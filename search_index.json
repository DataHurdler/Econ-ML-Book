[["index.html", "Machine Learning for Economics and Business Preface", " Machine Learning for Economics and Business Zijun Luo 2023-07-04 Preface The primary purpose of this book, which I started to write in May 2023, is to review, summarize, and organize what I have learned in Machine Learning as a PhD economist. I have taken several Machine Learning related courses on Coursera and Udemy. In 2022, I also worked, briefly, as a data scientist in an AdTech start-up. In my mind, I pretend that this book is written for those who have some quantitative training but is interested in what Machine Learning can offer, particularly graduate students who have taken at least one econometrics course. My emphases are those algorithms that can provide an alternative, sometimes more useful and rigorous, approach to known problems in economics, business, and social sciences. I also provide Python scripts that I have written to implement these algorithms. Please email me if you see any errors. "],["randomized-controlled-trial-abn-testing-and-multi-armed-bandit-algorithms.html", "Chapter 1 Randomized Controlled Trial, A/B/N Testing, and Multi-Armed Bandit Algorithms 1.1 Introduction 1.2 The Explore-Exploit Tradeoff 1.3 Epsilon Greedy 1.4 Optimistic Initial Values 1.5 Upper Confidence Bound (UCB) 1.6 Gradient Bandit Algorithm 1.7 Thompson Sampling (Bayesian Bandits) 1.8 Conjugate Prior 1.9 Thompson Sampling: Code 1.10 Comparing the Algorithms 1.11 Summary and Extensions 1.12 References", " Chapter 1 Randomized Controlled Trial, A/B/N Testing, and Multi-Armed Bandit Algorithms 1.1 Introduction Randomized Controlled Trial (RCT) is the gold standard for establishing causality in experimental methods. It is used widely in clinical trials for new drugs or field experiments in social sciences and economics. In business, especially e-commerce, a related concept is A/B/N Testing. The main idea of RCT and A/B/N test is straightforward: individuals are randomly divided into groups to receive different treatments. Afterwards treatments, outcomes are being valuated and compared in order to find out which treatment works better/best. In RCT, a control group, where individuals receive a “placebo”, is usually included. Note that placebo should be considered as a type of treatment too and individuals who receive a placebo are not getting “nothing”. A placebo is something that has no therapeutic effect, i.e., it is not designed to cure a disease or an illness. But it can nevertheless positively impact the well-being of individuals who have received it, if due to nothing but psychological effects. As a result, it would be rather wrong to expect “no effect” from the the controlled group that receives the placebo in an RCT. In the rest of this article, I will be using A/B/N test as the example because I want to stay away from the nitty-gritty details of RCT. I am using “A/B/N” to include tests with more than 2 versions. If you are only comparing two versions, it is an A/B test. When I was interviewing for a data scientist job in 2022, the following was one of the interview questions: We are going to run an A/B test on a client’s website. How long do we need to run the experiment for? Back then I knew about how to find minimum sample size based on hypothesis testing in Statistics, so I framed my answer that way. But I stopped in the middle while answering the question. Something I did not think seriously enough about popped into my head: how would I know the standard deviation, one of the required values to carry out the calculation for minimum sample size, before we even run the experiment? My interview went downhill from there. Needless to say, I did not get the job. However, the interviewer was nice enough to tell me that I should look into “power analysis”. I did. Suppose you have built an e-commerce website with two possible color pallettes, and you want to understand which color pallette would induce more purchases. You can randomly assign a visitor to the two versions of the website, and after a while, you will have a dataset with two columns: for each visitor, you recorded the version that the visitor was assigned to and the purchases that the visitor made. For \\(i\\in(A,B)\\), let’s define the following values: * \\(\\bar{x}_i\\): expected dollars spent by visitors of version \\(i\\); * \\(n_i\\): number of visitors who saw version \\(i\\); * \\(s_i\\): standard deviation of dollars spent by visitors who saw version \\(i\\). We can now calculate the “power” as \\[t=\\frac{\\bar{x}_A-\\bar{x}_B}{s_p\\sqrt{\\tfrac{1}{n_A}+\\tfrac{1}{n_B}}}\\] where \\(s_p=\\sqrt{\\frac{(n_A-1)s_A^2+(n_B-1)s_B^2}{n_A+n_B-2}}\\) is the pooled standard deviation. The “power”, \\(t\\), follows a \\(t\\)-distribution with \\(n_A+n_B-2\\) degrees of freedom. Suppose \\(s_A=s_B\\) for the two versions in your A/B test, we can denote the two standard deviations as \\(s\\). Also suppose, for simplicity, you want \\(n_A=n_B\\). You can solve for \\(n_i\\) from the above power analysis formula and obtain: \\[N=\\frac{4t^2s^2}{(\\bar{x}_A-\\bar{x}_B)^2}\\] where \\(N\\) is the total sample size (\\(n_A+n_B\\)). It is easy to see that you will need a larger sample size if * the expected difference between the two versions are smaller; * you want a better significance level, e.g., 1% instead of 5%; * the standard deviation is bigger, i.e., dollars spent are more dispersed among individuals; But here is the problem: you do not know the values of \\(\\bar{x}_i\\) and \\(s_i\\) before the experiment. For \\(\\bar{x}_i\\), it may be less of an issue. Instead of the expected values, all you really need is the expected difference, which can be specified. For example, suppose your website is currently running Version A, and all you care about is that Version B can increase expected purchase amount by 15. In other words, \\(\\bar{x}_B-\\bar{x}_A=15\\). But you still need to know the standard deviations. How? Some suggest that you can run a short trial to estimate the standard deviation. But then, isn’t the A/B test already a trial itself? Here is another problem about classic A/B test design. After I became a data scientist, at another company, we actually ran an A/B test. The problem is that, according to the aforementioned power analysis, the experiment needed to be ran for at least 3 months, but we did not have that much time. After 1 month, our model (Version B) outperformed the existed model (Version A). Could we have declared our model to be the better one? According to classic A/B test design, the answer is “No” because we should not be “peeking” as the difference can be driven by random factors happened only in the first month of the experiment. Now think about clinical trials for a new drug, where the “no peeking” rule can raise serious concerns. If a drug has proved its effectiveness in the first 500 patients, yet the power analysis tells you that you need to test it on 50,000 patients, what would you do? Isn’t it unethical to continue to give a placebo to individuals who may benefit from the actual drug? These two problems have bothered me for a while, until I learned about the approaches I will cover in this article. Here is a brief overview of how these algorithms work. Instead of having a predetermined sample size, the A/B test is deployed in real-time. Continued with our example of a website with two color pallettes, a visitor is randomly assigned to a version of the website on the first visit. An algorithm will then pick a version for a visitor. In general, the version that has received higher purchase values should get more visitors. But how do we know which one gives the higher payoff? Here, we face the Explore-Exploit Tradeoff. 1.2 The Explore-Exploit Tradeoff In a nutshell, the explore-exploit tradeoff shows the following paradox: in order to find the best version, you need to explore, which means that the outcome of exploration necessarily improves the more you different versions. However, to maximize total payoff, you want to stick with the best version once (you think) you have found it, which is to exploit. This means that the outcome of exploitation necessarily deteriorates the more you different versions since there is one and only one best version. How to handle the explore-exploit tradeoff constitutes the core difference among algorithms. Some algorithms, such as variants of the “greedy” family, really focuses on exploitation. The disadvantage is that such algorithm can easily “saddle” into the second-best as long as the second-best is “good enough”, as I will show later when we discuss the Epsilon Greedy algorithm. Others, such as Upper Confidence Bound, put more emphasis on exploration, at least initially. If you are reading this article because you think it may help you with your research project(s), you are not a stranger to the explore-exploit tradeoff. I remember a conversation I had with a professor not long after I graduated. I asked him if I should have given up on projects that I do not think that would end up in good journals. His answer was: but how do you know before you try? He had a point: my professor never published his PhD dissertation. He was successful after he has explored a new area of research. However, about 15 years after his dissertation, a paper on a closely related topic was published in a top journal. In retrospect, he may have explored more than the optimum, which was probably why he suggested me to exploit more. 1.3 Epsilon Greedy We will begin our in-depth discussion of algorithms with Epsilon Greedy. For each algorithm, I aim to provide: * intuition and description of theory * pseudocode * Python code Algorithms in the Greedy family applies a simple logic: choose the version that gives the best observed expected payoff. For simplicity, and for the rest of this article, let’s consider an e-commerce website that has 5 different designs but sells a single product: an EveryDay-Carry (EDC) musical instrument for 69.99 dollars. If we run an A/B/N test on the web designs, only 2 outcomes are possible from each visitor: buy or not buy. Here is the pseudocode for a simple Greedy algorithm: loop: j = argmax(expected bandit win rates) x = reward (1 or 0) from playing bandit j bandit[j].update_mean(x) I used bandit instead of version here, and will be using these two terms interchangeably, because the problem we are working on is commonly known as the Multi-Armed Bandits problem in probability theory and reinforcement learning. The analogy comes from choosing among multiple slot machines in a casino since a single slot machine is referred to as a “one-armed bandit”. Let’s take a closer look at the pseudocode. In this pseudocode, \\(i\\) indexes visitor, \\(j\\) indexes the website version (or bandit), and \\(x\\) is 1 when the visitor buys and 0 otherwise. Furthermore, update_mean() is a function that takes the new value of x and updates the expected payoff for bandit j. To update the expected payoff after bandit j was played for the \\(n_{th}\\) time, we have \\[\\bar{x}_n=\\bar{x}_{n-1}+\\frac{x_n-\\bar{x}_{n-1}}{n}\\] This calculates the mean in constant time and memory, i.e., it requires only 3 values to calculate the mean, \\(\\bar{x}_n\\), regardless of the value of \\(n\\): \\(\\bar{x}_{n-1}\\), \\(x_n\\), and \\(n\\), whereas the number of values required to calculate the mean with the formula \\[\\bar{x}_n=\\frac{\\sum_{i=1}^n{x_i}}{n}\\] increases with \\(n\\). While not necessary, it can sometimes be a good idea to try all versions in the beginning. For example, for the first 50 visitors, we can send them to each design with equal probability. From that point on, the algorithm finds the version that gives the best expected payoff, and play that version. It should be obvious that the simple Greedy algorithm has a problem: once it finds a bandit with a high enough payoff, it rarely switches. In other words, it almost never explores. The Epsilon Greedy algorithm provides a simple fix: loop: p = random number in [0, 1] if p &lt; epsilon: j = choose a bandit at random else: j = argmax(expected bandit win rates) x = reward (1 or 0) from playing bandit j bandit[j].update_mean(x) As the pseudocode shows, a random value is drawn when a new visitor has arrived. If the random value is smaller than the threshold, epsilon, set before the start of the experiment, then a random bandit is picked. Note that this randomly picked bandit can be the same as the one picked by argmax. To exclude such case only requires a few more lines of code. However, this is not a requirement of the Epsilon Greedy algorithm and the benefit of doing so is not obvious. Let’s now move onto the actual implementation of Epsilon Greedy in Python. Note that the script includes lines with the comment “only in demonstration”. These are codes to generate the true win rate of different bandits, which you do not know when running a real-world experiment. This also means you can not use the scripts here in real-world problems without making necessary changes. import numpy as np import random # set the number of bandits N_bandits = 5 # set the number of trials # only in demonstration N = 100000 class BayesianAB: def __init__( self, number_of_bandits: int = 2, number_of_trials: int = 100000, p_max: float = .75, p_diff: float = .05, p_min: float = .1 ): if p_min &gt; p_max - p_diff: raise ValueError(&quot;Condition p_min &lt; p_max - p_diff not satisfied. Exit...&quot;) self.prob_true = [0] * number_of_bandits # only in demonstration self.prob_win = [0] * number_of_bandits self.history = [] self.history_bandit = [] # for Monte Carlo self.count = [0] * number_of_bandits # only in demonstration # preference and pi are for gradient_bandit only self.pref = [0] * number_of_bandits self.pi = [1 / number_of_bandits] * number_of_bandits # a and b are for bayesian_bandits only self.alpha = [1] * number_of_bandits self.beta = [1] * number_of_bandits # number of trials/visitors self.N = number_of_trials # set the last bandit to have a win rate of 0.75 and the rest lower # only in demonstration self.prob_true[-1] = p_max for i in range(0, number_of_bandits - 1): self.prob_true[i] = round(p_max - random.uniform(p_diff, p_max - p_min), 2) # Receives a random value of 0 or 1 # only in demonstration def pull( self, i, ) -&gt; bool: return random.random() &lt; self.prob_true[i] # Updates the mean def update( self, i, k, ) -&gt; None: outcome = self.pull(i) # may use a constant discount rate to discount past self.prob_win[i] = (self.prob_win[i] * k + outcome) / (k + 1) self.history.append(self.prob_win.copy()) self.history_bandit.append(i) # for Monte Carlo self.count[i] += 1 #################### # epsilon greedy def epsilon_greedy( self, epsilon: float = 0.5, ): self.history.append(self.prob_win.copy()) for k in range(1, self.N): if random.random() &lt; epsilon: i = random.randrange(0, len(self.prob_win)) else: i = np.argmax(self.prob_win) self.update(i, k) return self.history Let’s break it down. First, we import two libraries: numpy and random. We will be using functions from these libraries such as argmax() from numpy and randrange() from random: import numpy as np import random We then set three global parameters: # set the number of bandits N_bandits = 5 # set the number of trials/visitors N = 100000 In practice, the value of N_bandits depends on the number of versions your experiment is set out to test, and the number of visitors, N, is unknown. In this script, we are creating a class named BayesianAB, and put all the algorithms we cover in this article under BayesianAB. We initiate the class with the following values: class BayesianAB: def __init__( self, number_of_bandits: int = 2, number_of_trials: int = 100000, p_max: float = .75, p_diff: float = .05, p_min: float = .8 ): if p_min &gt; p_max - p_diff: raise ValueError(&quot;Condition p_min &lt; p_max - p_diff not satisfied. Exit...&quot;) self.prob_true = [0] * number_of_bandits # only in demonstration self.prob_win = [0] * number_of_bandits self.history = [] self.history_bandit = [] # for Monte Carlo self.count = [0] * number_of_bandits # only in demonstration # preference and pi are for gradient_bandit only self.pref = [0] * number_of_bandits self.pi = [1 / number_of_bandits] * number_of_bandits # a and b are for bayesian_bandits only self.alpha = [1] * number_of_bandits self.beta = [1] * number_of_bandits # number of trials/visitors self.N = number_of_trials The BayesianAB class accepts 5 parameters: * number_of_bandits has a default value of 2; * number_of_trials indicates the number of rounds/visitors, which has a default value of 100,000; * p_max is the highest win rate; * p_diff is the smallest possible difference between the highest win rate and the second highest win rate; * p_min is the lowest possible win rate, andt the condition p_min &gt; p_max - p_diff must be met. The BayesianAB class pre-allocates 10 lists to store various values necessary for different tasks: * prob_true stores the true win rate of each bandit. These win rates are to be generated next. In practice, you do not know these true win rate values; * prob_win stores the observed (expected) win rate of each bandit. Values in this list are to be updated during each round of the experiment; * history stores the history of prob_win in each trial/round. This is important for both updating the mean in constant time (see above) and the evaluation of bandit/algorithm performances afterwards; * history_bandit stores the history of what bandit was picked in each trial/round. This is useful when we need to run the Monte Carlo simulation for testbed; * count stores the number of times that each bandit was chosen; * pref and pi are values for the Gradient Bandit algorithm; * alpha and beta are values used in Thompson Sampling, the last algorithm to be considered in this article; * N stores the number of trials and is used by each method/algorithm in the BayesianAB class. The following lines generate the true win rates: # set the last bandit to have a win rate of 0.75 and the rest lower # only in demonstration self.prob_true[-1] = p_max for i in range(0, number_of_bandits - 1): self.prob_true[i] = round(p_max - random.uniform(p_diff, p_max - p_min), 2) The last bandit always has the highest win rate, p_max, and the rest of them are randomized between p_min and p_max - p_diff. I used this approach to allow for flexibility in specifying the number of bandits using N_bandits (or number_of_bandits inside the BayesianAB class). Next, we define two functions used by almost every algorithm: # Receives a random value of 0 or 1 # only in demonstration def pull( self, i, ) -&gt; bool: return random.random() &lt; self.prob_true[i] # Updates the mean def update( self, i, k, ) -&gt; None: outcome = self.pull(i) # may use a constant discount rate to discount past self.prob_win[i] = (self.prob_win[i] * k + outcome) / (k + 1) self.history.append(self.prob_win.copy()) self.history_bandit.append(i) # for Monte Carlo self.count[i] += 1 The first function, pull(), returns either True or False depending on if the value of random.random() is less than the true win rate of bandit \\(i\\). This is unnecessary in practice. Instead, a call to either the BayesianAB class or a specific method (such as Epsilon Greedy) inside BayesianAB should be triggered with the arrival of a new visitor, and by the end of the visit, you would know if the visitor has purchased (True) or not (False). In Python, True is given a numerical value of 1 and False a value of 0. The update() function updates the mean. It also adds the updated expected win rate to the list history and increase the count of bandit \\(i\\) being picked by 1. Here is the actual method inside BayesianAB that implements epsilon greedy: def epsilon_greedy( self, epsilon: float = 0.5, ): self.history.append(self.prob_win.copy()) for k in range(1, self.N): if random.random() &lt; epsilon: i = random.randrange(0, len(self.prob_win)) else: i = np.argmax(self.prob_win) self.update(i, k) return self.history It follows the logic outlined in the pseudocode. Inside the for loop, these steps are followed: 1. Checks if a random value is smaller than epsilon which can be specified when the epsilon_greedy() method is called. epsilon also has a default value of \\(0.5\\). If this is True, then a random bandit is selected; 2. Otherwise, select the bandit that has the highest expected win rate; 4. Update the mean for the chosen bandit by calling the update() function. The epsilon_greedy() method returns the list history, which stores the complete history for run as discussed earlier. To call epsilon_greedy() and examine the results, we execute the following: eg = BayesianAB(N_bandits) print(f&#39;The true win rates: {eg.prob_true}&#39;) eg_history = eg.epsilon_greedy() print(f&#39;The observed win rates: {eg.prob_win}&#39;) print(f&#39;Number of times each bandit was played: {eg.count}&#39;) Here, we call epsilon_greedy() with the default value for epsilon. This means the algorithm will explore half of the time. We also print out the true win rates, the expected win rates, and the number of times that each bandit was played. Here is the printed output from a particular run: The true win rates: [0.37, 0.55, 0.67, 0.4, 0.75] The observed win rates: [0.2062, 0.3354, 0.6717, 0.1953, 0.5526] Number of times each bandit was played: [10200, 9945, 60001, 9789, 10064] In the above run, the best bandit was NOT the one that was selected the most. The second best bandit, with a 0.67 win rate, was picked about 60% of the time, as dictated by the value of epsilon. Such outcome is due to the fact that the bandit with a 0.67 win rate did well in the beginning. Since it is close enough to 0.75, the default win rate of the best bandit, random jumps to the bandit with the 0.75 win rate were not enough to “flip” the results. Also note that the expected win rates have not converged to the true win rates except for the “chosen” one after 100,000 visitors. However, if the number of visitors approaches infinity, which means that each version would be picked infinite times, all win rates would converge to their true values. This, in turn, means that the best bandit would eventually overtake the second-best if the experiment runs long enough. In other words, Epsilon Greedy guarantees the identification of the best bandit as \\(n\\) approaches infinity. We can visualize the outcome from the experiment with the following code: import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def plot_history( history: list, prob_true: list, k=N, ): df_history = pd.DataFrame(history[:k]) plt.figure(figsize=(20, 5)) # Define the color palette colors = sns.color_palette(&quot;Set2&quot;, len(prob_true)) for i in range(len(prob_true)): sns.lineplot(x=df_history.index, y=df_history[i], color=colors[i]) # Create custom legend using prob_true and colors custom_legend = [plt.Line2D([], [], color=colors[i], label=prob_true[i]) for i in range(len(prob_true))] plt.legend(handles=custom_legend) Then execute: plot_history(history=eg.history, prob_true=eg.prob_true) Here is the output from the above run: Epsilon Greedy We can also get the visualization for the first 100 visitors, which shows that the third bandit, the a 0.67 win rate, jumped ahead early: plot_history(history=eg.history, prob_true=eg.prob_true, k=100) Epsilon Greedy (first 100) 1.4 Optimistic Initial Values The Optimistic Initial Values algorithm is one of my favorites (the other being the Gradient Bandit algorithm) amongst the algorithms discussed in this article. While Epsilon Greedy focused on “exploit” and can end up choosing the second-best version, the Optimistic Initial Values algorithm puts more focus on “explore” initially, while staying greedy, i.e., pick the strategy that shows the highest expected win rate. The name of this algorithm informs you about what it does: at the start of the experiment, each bandit is set to have a high expected win rate, i.e., we are “optimistic” about each bandit. This ensures that each of them is played a fair number of times initially. If we compare Epsilon Greedy to English auctions where the values go up over time, Optimistic Initial Value is comparable to Dutch auctions where the values go down over time. Here is the pseudocode: p_init = 5 # a large value as initial win rate for ALL bandits loop: j = argmax(expected bandit win rates) x = reward (1 or 0) from playing bandit j bandit[j].update_mane(x) Assuming you already have the code from the Epsilon Greedy section, you can add the following method inside the BayesianAB class to run the Optimistic Initial Values algorithm: #################### # optimistic initial values def optim_init_val( self, init_val: float = 0.99, ): self.prob_win = [init_val] * len(self.prob_win) self.history.append(self.prob_win.copy()) for k in range(1, self.N): i = np.argmax(self.prob_win) self.update(i, k) return self.history The only thing new here is the line that assigns init_val to prob_win in the beginning. We can execute the following to get results and visualization: oiv = BayesianAB(N_bandits) print(f&#39;The true win rates: {oiv.prob_true}&#39;) oiv_history = oiv.optim_init_val(init_val=0.99) print(f&#39;The observed win rates: {oiv.prob_win}&#39;) print(f&#39;Number of times each bandit was played: {oiv.count}&#39;) # plot the entire experiment history plot_history(history=oiv.history, prob_true=oiv.prob_true) And following are outcomes from a typical run: The true win rates: [0.6, 0.54, 0.62, 0.14, 0.75] The observed win rates: [0.6633, 0.7493, 0.7491, 0.7493, 0.7521] Number of times each bandit was played: [2, 168, 285, 65, 99479] Optimistic Initial Values From the visualization below, you can see that the best bandit jumped ahead after about merely 15 visitors: Optimistic Initial Values (first 100) Note that I set init_val to 0.99 since we are comparing win rates that can not exceed 1. The larger the initial value, the more the algorithm explores initially. Because the Optimistic Initial Values algorithm was specifically designed to explore in the beginning, it can “fall behind” in reaching the best version, if ever, compared to other algorithms such as Epsilon Greedy. Note that if the best bandit is discovered early, the expected win rates of other bandits never converge to their true win rates in Optimistic Initial Values (but would in Epsilon Greedy). This is a common feature of several of the algorithms discussed in this article. 1.5 Upper Confidence Bound (UCB) The theory of UCB is harder to fully grasp although its intuition and implementation are straightforward. To start, let’s look back to the last two algorithms that we have discussed: Epsilon Greedy and Optimistic Initial Values. A common step in the implementation of both of these algorithms is to find the bandit that gives the best observed expected win rate. This is why both algorithms are said to be greedy. But can we do better, especially that we know these expected win rates are probabilistic? Put differently, we know that the more a certain bandit was selected, the closer its expected win rate is to its true win rate. But what about those bandits that were rarely picked? That is where Upper Confidence Bound comes into play. The idea is that we should not be relying on the observed expected win rates alone in making decisions. We should give each version some “bonus points”: if a version has been picked a lot, it gets a small bonus; but if a version has barely been barely chosen, it should get a larger bonus because, probabilistically, the observed expected win rate can be far from the true win rate if a version has not been played much. If you are interested in the math, you can read the paper “Finite-time Analysis of the Multiarmed Bandit Problem”. In the paper, the authors have outlined a function for the “bonus”, which is commonly known as UCB1: \\[b_j=\\sqrt{\\frac{2\\log{N}}{n_j}}\\] where \\(N\\) is the total number of visitors at the time of computing the bonus, and \\(n_j\\) is the number of times that bandit \\(j\\) was chosen at the time of computing the bonus. Adding \\(b\\) to the expected win rate gives the upper confidence bound: \\[\\text{UCB1}_j=\\bar{x}_{n_j}+b_j\\] Here is the pseudocode for UCB1: loop: Update UCB1 values j = argmax(UCB1 values) x = reward (1 or 0) from playing bandit j bandit[j].update_mean(x) Adding the following method into BayesianAB will implement UCB1: #################### # upper confidence bound (UCB1) def ucb1( self, c=1, ): self.history.append(self.prob_win.copy()) bandit_count = [0.0001] * len(self.prob_win) for k in range(1, self.N): bound = self.prob_win + c * np.sqrt(np.divide(2 * np.log(k), bandit_count)) i = np.argmax(bound) self.update(i, k) if bandit_count[i] &lt; 1: bandit_count[i] = 0 bandit_count[i] += 1 return self.history This is very similar to what we had before. One thing to note is that I give a very small initial value (\\(0.0001\\)) to bandit_count to avoid the division of zero in the beginning of the experiment. Later, I reversed the value to 0 with the if statement. An alternative approach is to run the first several iterations on all versions before implementing UCB1 thereafter. UCB1 has a parameter, \\(c\\), which controls the degree of exploration. Other things being equal, A larger value \\(c\\) means a higher reward. The default value is set to 1 in the above script. Executing the following will give us results and visualization for UCB1: ucb = BayesianAB(N_bandits) print(f&#39;The true win rates: {ucb.prob_true}&#39;) ucb_history = ucb.ucb1() print(f&#39;The observed win rates: {ucb.prob_win}&#39;) print(f&#39;Number of times each bandit was played: {ucb.count}&#39;) # plot the entire experiment history plot_history(history=ucb.history, prob_true=ucb.prob_true) A typical run gives the following: The true win rates: [0.65, 0.12, 0.63, 0.33, 0.75] The observed win rates: [0.6505, 0.1165, 0.1928, 0.0774, 0.3794] Number of times each bandit was played: [99470, 77, 103, 67, 282] UCB1 This particular run shows that UCB1 has also failed to identify the best version. Examining the first 100 visitors shows that the bandit with a .65 win rate jumped out early and never looked back. And I do not believe the algorithm can guarantee convergence even with infinite number of visitors: UCB1 (first 100) 1.6 Gradient Bandit Algorithm Another algorithm that does not rely entirely on expected win rates is the Gradient Bandit algorithm. Not surprisingly, this algorithm is related to gradient ascent. With this algorithm, each bandit’s probability of being chosen is determined according to a soft-max distribution: \\[\\pi_n(i)=\\frac{e^{H_n(i)}}{\\sum_{j=1}^{J}{e^{H_n(j)}}}\\] where \\(\\pi_n(i)\\) is the probability of bandit \\(i\\) being picked for customer \\(n\\), \\(H_n(i)\\) is the preference for bandit \\(i\\) at the time customer \\(n\\) arrives, and \\(J\\) is the total number of bandits in the experiment. In the case of only two bandits, this specification is the same as the logistic or sigmoid function. When the first customer arrives, i.e., \\(n=1\\), it is custom to set the preference, \\(H_1(j)\\), for all \\(j\\), to 0 so that every bandit has the same probability of getting picked. Suppose bandit \\(i\\) is picked for customer \\(n(\\geq1)\\), then the preference for \\(i\\) is updated according to: \\[H_{n+1}(i)=H_n(i)+\\alpha(x_n - \\bar{x}_{n-1})(1-\\pi_n(i))\\] whereas the preferences for all \\(j\\neq i\\) are updated according to: \\[H_{n+1}(j)=H_n(j)-\\alpha(x_n - \\bar{x}_{n-1})\\pi_n(j)\\] where \\(\\alpha&gt;0\\) is a “step-size” parameter. The intuition of the Gradient Bandit algorithm is as follows. When the reward received from picking \\(i\\) for customer \\(n\\) is higher than the expected reward, the probability of picking \\(i\\) in the future (next round) is increased. In our simple case with only two outcomes (buy and not buy), the reward is higher than the expected reward only if customer \\(n\\) buys. Let’s take a look at the pseudocode: H = [0] * number_of_bandits loop: pi = pi(H) # Calculates the soft-max distribution i = random.choices(bandits, weights=pi) gb.update() where H.update() updates the values of \\(H(i)\\) (the bandit that was chosen) and \\(H(j)\\) (bandits that were not chosen). Here is the Python implementation for Gradient Bandit: #################### # gradient_bandit update def gb_update( self, i, k, a, ): outcome = self.pull(i) for z in range(len(self.pref)): if z == i: self.pref[z] = self.pref[z] + a * (outcome - self.prob_win[z]) * (1 - self.pi[z]) else: self.pref[z] = self.pref[z] - a * (outcome - self.prob_win[z]) * self.pi[z] self.prob_win[i] = (self.prob_win[i] * k + outcome) / (k + 1) return self.pref # gradient bandit algorithm def gradient_bandit( self, a=0.2, ): self.history.append([self.pi.copy(), self.pref.copy(), self.prob_win.copy()]) for k in range(1, self.N): self.pi = np.exp(self.pref) / sum(np.exp(self.pref)) pick = random.choices(np.arange(len(self.pref)), weights=self.pi) i = pick[0] self.pref = self.gb_update(i, k, a) self.count[i] += 1 self.history.append([self.pi.copy(), self.pref.copy(), self.prob_win.copy()]) self.history_bandit.append(i) # for Monte Carlo return self.history Here are some notes on the Python implementation of the Gradient Bandit algorithm: 1. We have already initiated pref and pi at the start of the BayesianAB class; 2. As discussed in the pseudocode, a new function, gb_update(), is necessary since we need to update the preference function for every bandit in each round; 3. The gradient_bandit() function takes 1 parameter: \\(a\\), which is the step-size parameter. The default value of \\(a\\) is set to 0.2. The smaller the value of \\(a\\), the more the algorithm explores; 4. gradient_bandit() saves history differently: each row in history is an array of 3 lists. In order to examine the performance of the Gradient Bandit algorithm, we not only save the expected win rates, but also preferences and the values from the soft-max distribution, \\(\\pi(i)\\); 5. The function choices() from the random library picks a value from a list based on weights. In this case, the weights is given by the soft-max distribution; Because gradient_bandit() saves arrays in history, we also need to update the plot_history() function: def plot_history( history: list, prob_true: list, col=2, k=N, ): if type(history[0][0]) == list: # To accommodate gradient bandit df_history = pd.DataFrame([arr[col] for arr in history][:k]) else: df_history = pd.DataFrame(history[:k]) plt.figure(figsize=(20, 5)) # Define the color palette colors = sns.color_palette(&quot;Set2&quot;, len(prob_true)) for i in range(len(prob_true)): sns.lineplot(x=df_history.index, y=df_history[i], color=colors[i]) # Create custom legend using prob_true and colors custom_legend = [plt.Line2D([], [], color=colors[i], label=prob_true[i]) for i in range(len(prob_true))] plt.legend(handles=custom_legend) The updates occurred in if type(history[0][0]) == list: # To accommodate gradient bandit df_history = pd.DataFrame([arr[col] for arr in history][:k]) else: df_history = pd.DataFrame(history[:k]) with the added parameter col. This is to accommodate the arrays saved in history by gradient_bandit(). The if statement checks whether the first element in history is a list. If it is, then history was saved from gradient_bandit() and we would need to extract the specific column, given by col, for plotting. The default value of col is 2, which is to plot the history of the win rates. Executing the following will run the Gradient Bandit algorithm: # Gradient bandit gb = BayesianAB(N_bandits) print(f&#39;The true win rates: {gb.prob_true}&#39;) gb_history = gb.gradient_bandit() print(f&#39;The observed win rates: {gb.prob_win}&#39;) print(f&#39;Number of times each bandit was played: {gb.count}&#39;) # plot the entire experiment history plot_history(history=gb.history, prob_true=gb.prob_true) Here are results from a typical run: The true win rates: [0.17, 0.56, 0.17, 0.7, 0.75] The observed win rates: [0.2564, 0.5514, 0.0105, 0.6636, 0.7498] Number of times each bandit was played: [35, 67, 22, 196, 99679] Gradient Bandit As usual, we can examine what happened after 100 customers. Interestingly, the bandit with the highest win rate did not lead after only 100 customers: Gradient Bandit (first 100) We can plot the evolution of the preference with the following: # plot preference plot_history(history=gb.history, prob_true=gb.prob_true, col=1) Gradient Bandit (preference) And plot the soft-max function with the following: # plot pi plot_history(history=gb.history, prob_true=gb.prob_true, col=0) Gradient Bandit (pi) There are several reasons why the Gradient Bandit algorithm is one of my favorites: 1. Economists are familiar with the idea of using preference to model choices; 2. Economists are familiar with logistic function, used in logistic regression, which is the special case of soft-max with only two bandits; 3. One of my research areas is conflict and contest, in which the soft-max function, known as “contest success function”, is widely used in the literature. 1.7 Thompson Sampling (Bayesian Bandits) Thompson Sampling, or Bayesian Bandits, takes another (big) step forward. In our discussion on Upper Confidence Bound, we acknowledged the fact that using only the expected win rate to represent the performance of a bandit is not accurate. To tackle this, UCB1 adds a “bonus”: the bonus is smaller for the bandits that were played more, and larger for the bandits that were played less. Then in our discussion on Gradient Bandit, each bandit’s chance of being picked is described by a soft-max distribution. To push these ideas further, and as the name Thompson Sampling has hinted, we ask if we could construct a probability distribution to describe the expected win rates of all the bandits. As it turns out, this is possible, as everything, including parameters, are considered random variables in Bayesian Statistics. For example, with Normal Distribution, we often speak about fixed values of mean and variance. But in Bayesian Statistics, the mean and variance of a Normal Distribution are two random variables and they can be described by probability distributions. The mathematical derivation of Thompson Sampling requires the use of conjugate prior, which I will discuss here briefly, before returning to the Python implementation of Thompson Sampling. 1.8 Conjugate Prior Recall the Bayes Rule: \\[p(\\theta \\mid X)=\\frac{p(X \\mid \\theta)p(\\theta)}{p(X)}\\] where the four parts are called, respectively * \\(p(\\theta \\mid X)\\): Posterior distribution * \\(p(X \\mid \\theta)\\): Likelihood function * \\(p(\\theta)\\): Prior probability distribution * \\(p(X)\\): Evidence In Bayesian Statistics, if the posterior distribution is in the same probability distribution family as the prior probability distribution, the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. With conjugate priors, the updating in a Bayesian approach reduces to the updates of hyperparameters that are used to describe both the prior and posterior distributions, since they are the same. I will leave the details to a Statistics textbook, but for our purpose, since our example concerns of binary outcomes (buy or not buy), our likelihood function is that of Bernoulli. As it turns out, the conjugate prior for a Bernoulli likelihood function is the Beta distribution, which has two hyperparameters: \\(\\alpha\\) and \\(\\beta\\). Now that we have established that Beta distribution is the conjugate prior for Bernoulli, the problem of Thompson Sampling is reduced to 1. sample from the Beta distribution 2. find the highest expected value 1.9 Thompson Sampling: Code Since Thompson Sampling is mechanical different from the previous algorithms, we need to develop special functions and methods to implement Thompson Sampling. Here is a pseudocode: loop: sampling from Beta function for bandit b j = argmax(b.sample() for b bandits) x = reward (1 or 0) from playing bandit j bandit[j].bb_update(x) The function that needs to be added is bb_update(). Here is the full Python implementation: from scipy.stats import beta #################### # bayesian_bandits update def bb_update( self, a, b, i, ): outcome = self.pull(i) a[i] += outcome b[i] += 1 - outcome self.count[i] += 1 return a, b # Bayesian bandits # For Bernoulli distribution, the conjugate prior is Beta distribution def bayesian_bandits( self, sample_size: int = 10, ): a_hist, b_hist = [], [] a_hist.append(self.alpha.copy()) b_hist.append(self.beta.copy()) for k in range(1, self.N): sample_max = [] for m in range(len(self.prob_true)): m_max = np.max(np.random.beta(self.alpha[m], self.beta[m], sample_size)) sample_max.append(m_max.copy()) i = np.argmax(sample_max) self.alpha, self.beta = self.bb_update(self.alpha, self.beta, i) a_hist.append(self.alpha.copy()) b_hist.append(self.beta.copy()) self.history_bandit.append(i) # for Monte Carlo self.history = [a_hist, b_hist] return self.history Let’s walk through this script: 1. We have already initiated alpha and beta at the start of the BayesianAB class. They are the hyperparameters in the Beta distribution; 2. We import beta from scipy.stats since the conjugate prior for Bernoulli distribution is the Beta distribution. 3. The function bb_update() updates the hyperparameter values based on the outcome from the last visitor for bandit \\(i\\): if the outcome was True, then the value of alpha increases by 1; otherwise, the value of beta increases by 1. For the actual implementation of the Bayesian Bandits in the bayesian_bandits() method, it is largely consistent with what we have been doing in other algorithms. The main differences include: 1. Instead of storing the history of outcomes, we store the history of the values of alpha and beta; 2. In each iteration, we first find the maximum value from the sample of values of each bandit, then pick the best from this set of maximum values; 3. As described earlier, the updating is different. Instead of updating the mean, we update the values of alpha and beta. Due to these changes, we also need a new function for visualizing the history returned by bayesian_bandits(): def bb_plot_history( history: list, prob_true: list, k=-1, ): x = np.linspace(0, 1, 100) legend_str = [[]] * len(prob_true) plt.figure(figsize=(20, 5)) for i in range(len(prob_true)): a = history[0][k][i] b = history[1][k][i] y = beta.pdf(x, a, b) legend_str[i] = f&#39;{prob_true[i]}, alpha: {a}, beta: {b}&#39; plt.plot(x, y) plt.legend(legend_str) We can now run a simulate for Thompson Sampling by executing the following: bb = BayesianAB(N_bandits) print(f&#39;The true win rates: {bb.prob_true}&#39;) bb_history = bb.bayesian_bandits(sample_size=10) print(f&#39;The observed win rates: {np.divide(bb.history[0][-1], bb.count)}&#39;) print(f&#39;Number of times each bandit was played: {bb.count}&#39;) # plot the entire experiment history bb_plot_history(history=bb.history, prob_true=bb.prob_true) Outcomes from a typical run look like the following: The true win rates: [0.15, 0.67, 0.11, 0.47, 0.75] The observed win rates: [0.1, 0.6563, 0.1667, 0.4545, 0.7500] Number of times each bandit was played: [10, 355, 12, 44, 99578] Bayesian Bandits It is also interesting to look at what happened after only 100 visitors: Bayesian Bandits (first 100) Two differences between Thompson Sampling and the other algorithms we have discussed should be noted. First, as already mentioned, Thompson Sampling attempts to build a distribution for the bandits. Comparing the two visuals from 100 visitors and all visitors shows that, although the best version has jumped out early, the distribution is much tighter/narrower at the end of the experiment, indicating greater “confidence” for the estimated expected win rate. Second, and importantly, the Thompson Sampling algorithm has no problem distinguishing between a bandit with a 0.67 win rate and the best version with a win rate of 0.75. 1.10 Comparing the Algorithms It is important to compare the five algorithms in various settings. Following Sutton and Barto (2020), I conduct a 5-armed testbed. The idea of the testbed is to run the algorithms many times, say 2,000, then calculates the success rate, which is the percentage that the best bandit was picked in each round. For example, suppose we run the Epsilon Greedy algorithm 2,000 times with different win rates. We look at the bandit picked on the 100th visitor and found that, out of the 2,000 runs the best bandit was picked 800 times. Then at the 100th round/visitor, the success rate was 0.4. When we developed the BayesianAB class, we were already anticipating the implementation of the testbed. Specifically, in the following functions/methods: * update() * gradient_bandit() * bayesian_bandits() there is self.history_bandit.append(i) which records the which bandit was picked at each round. The parameters p_max, p_diff, and p_min also allow the BayesianAB class to generate different win rates. We will now develop a script to implement the testbed where the BayesianAB class is imported and called: import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from multiprocessing import Pool, cpu_count from functools import partial from bayesianab import BayesianAB # set the number of bandits N_bandits = 5 # set the number of visitors N = 10001 # set the number of trials M = 2000 def worker(algo, number_of_bandits, number_of_trials, p_max, p_diff, p_min, n): bayesianab_instance = BayesianAB(number_of_bandits, number_of_trials, p_max, p_diff, p_min) getattr(bayesianab_instance, algo)() return bayesianab_instance.history_bandit def monte_carlo( algos, m=500, n=10001, p_max: float = .75, p_diff: float = .05, p_min: float = .1 ): algos_hist = {algo: [] for algo in algos} for algo in algos: print(f&#39;Running {algo}...&#39;) with Pool(cpu_count()) as pool: func = partial(worker, algo, N_bandits, n, p_max, p_diff, p_min) results = list(pool.imap(func, range(m))) algos_hist[algo] = results return algos_hist def run_monte_carlo( algos, m, n, p_values, ): trials = {} df_all = {} for i in range(len(p_values)): print(f&#39;The p_values are {p_values[i]}&#39;) trials[f&#39;p{i}&#39;] = monte_carlo(algos, m, n, p_values[i][0], p_values[i][1], p_values[i][2],) for i in range(len(p_values)): df = pd.DataFrame() for j in algos: lst = [0] * (N - 1) for k in range(M): lst = np.array(lst) + np.array([1 if x == 4 else 0 for x in trials[f&#39;p{i}&#39;][j][k]]) df[j] = (lst / M).tolist() df_all[f&#39;p{i}&#39;] = df.copy() return df_all def plot_monte_carlo( df_all, algos, col, row, ): figure, axis = plt.subplots(row, col, figsize=(20, 10)) colors = sns.color_palette(&quot;Set2&quot;, len(algos)) m = 0 # column index n = 0 # row index for key in df_all: ax = axis[n, m] for i in range(len(algos)): sns.lineplot(x=df_all[key].index, y=df_all[key][algos[i]], linewidth=0.5, color=colors[i], ax=ax) ax.set_ylabel(&#39;&#39;) ax.set_title(prob_list[n * 3 + m]) ax.set_xticks([]) if m == 2: # Create custom legend using prob_true and colors custom_legend = [plt.Line2D([], [], color=colors[i], label=algos[i]) for i in range(len(algos))] ax.legend(handles=custom_legend, loc=&#39;upper left&#39;, fontsize=9) n += 1 m = 0 else: m += 1 figure.suptitle(&#39;Comparing 5 Algorithms in 12 Different Win Rate Specifications&#39;, fontsize=16) # Adjust the spacing between subplots plt.tight_layout() plt.savefig(&quot;comparison.png&quot;, dpi=300) # plt.show() if __name__ == &quot;__main__&quot;: algorithms = [&#39;epsilon_greedy&#39;, &#39;optim_init_val&#39;, &#39;ucb1&#39;, &#39;gradient_bandit&#39;, &#39;bayesian_bandits&#39;] prob_list = [[.35, .1, .1], [.35, .05, .1], [.35, .01, .1], [.75, .1, .1], [.75, .05, .1], [.75, .01, .1], [.75, .1, .62], [.75, .05, .62], [.75, .01, .62], [.95, .1, .82], [.95, .05, .82], [.95, .01, .82], ] results_df = run_monte_carlo(algorithms, M, N, prob_list) plot_monte_carlo(results_df, algorithms, 3, 4) Some explanations may be instructive. First, since we will be calling the same functions many times, I decided to use parallelization, which is through the multiprocessing library. In the script, the worker() function defines the task (or worker) for parallelization. The core function in the script, monte_carlo(), accepts six arguments: 1. algos contains a list of algorithms. The algorithms should match the names given as methods in the BayesianAB class. In our case, we have algorithms = [&#39;epsilon_greedy&#39;, &#39;optim_init_val&#39;, &#39;ucb1&#39;, &#39;gradient_bandit&#39;, &#39;bayesian_bandits&#39;] m is the number of simulations/trials to run. The default value is 500. We will actually be running 2000 simulations; n is the number of rounds/visitors in each simulation. A default value of 10001 means it will have 10000 visitors; p_max is the highest win rate; p_diff is the smallest possible difference between the highest win rate and the second highest win rate; p_min is the lowest possible win rate. We will run simulations with 12 different combinations of p_max, p_diff, and p_min, given in the following list: prob_list = [[.35, .1, .1], [.35, .05, .1], [.35, .01, .1], [.75, .1, .1], [.75, .05, .1], [.75, .01, .1], [.75, .1, .62], [.75, .05, .62], [.75, .01, .62], [.95, .1, .82], [.95, .05, .82], [.95, .01, .82], ] The run_monte_carlo() function calls the monte_carlo() function, then processes the results and calculate success rate in each round. The results are stored in a dictionary named df_all. The plot_monte_carlo() function, as the name suggests, plots the results in a 4-by-3 grid. Each subplot corresponds to a certain combination of [p_max, p_diff, p_min] which is the titles of the subplots. Here is the resulted plot with 2,000 simulations, each with 10,000 rounds/visitors: Comparison Several results are worth mentioning: 1. Thompson Sampling and Gradient Bandit both do consistently well. Thompson Sampling has the best overall performance, picking the best bandit in over 90% of the simulations at the end of 10,000 rounds. It also edged out Gradient Bandit when p_diff is 0.01, which means there can be a close second-best bandit; 2. The Epsilon Greedy algorithm performs consistently regardless of win rate settings, but at a poor 20% success rate at picking the best bandit. This may be partly due to a relatively high value of epsilon at 0.5; 3. The algorithm that is the most sensitive to win rate settings is UCB1. When the win rate of the best bandit is at 0.95, UCB1’s performance can be puzzling. In part, it may be a the result of a relatively high c value, since the default is 1. Intuitively, when the best bandit has a win rate of 0.95, and especially when there exists a close second-best, the “bonus” that UCB1 can give to a bandit is small. After all, the win rate is not to be exceeding 1. As a result, UCB1 has a hard time distinguishing between the best bandits and others that are almost as good. It should be noted that UCB (not UCB1) has the best performance in the testbed in Sutton and Barto (2020), but the authors also acknowledged that UCB’s application beyond the Multi-Armed Bandit problem is limited (in the context of reinforcement learning). 1.11 Summary and Extensions In this article, I have introduced five algorithms that can be used in real-time A/B Testing and Randomized Controlled Trials. Compared to traditional methods that often use Power Analysis to determine the minimum sample size, algorithms introduced in this article have one additional advantage: They can be easily extended to experiments with more than 2 choices/versions/bandits, as shown throughout the article. The algorithms introduced in this article are known as algorithms for the Multi-Armed Bandit problem, which is considered as the simplest form of reinforcement learning. We will come back to more reinforcement learning algorithms and problems in later articles. Two extensions of the Multi-Armed Bandit problem should be mentioned: Non-stationary Bandit and Contextual Bandit. Non-stationary Bandit is the situation in which that the win rates change over time. One particular algorithm that we introduced in this article is known to do badly in non-stationary bandit problems: Optimistic Initial Values. The reason is obvious. Optimistic Initial Values algorithm is designed to explore aggressively in the beginning. When the win rates change after this initial exploration stage has ended, it is hard for it to change course. As the name suggested, Contextual Bandit means that there exists contextual information to be used when making a decision. In a simple example, a casino may have slot machines in different colors, and the colors are not random: the green machines have higher win rates than the red ones. A new player would not know that information at first, but as time goes on and if the player has explored enough, it is possible to figure out the association between color and win rate, and hence decisions are made accordingly. This is also why Contextual Bandit is also known as Associative Search. We will come back to this in another article, since Contextual Bandit algorithms can be used in regression problems. (Visit my GitHub for the latest Python scripts: https://github.com/DataHurdler/Econ-ML/tree/main/Multi-Arm%20Bandits) 1.12 References https://www.udemy.com/course/bayesian-machine-learning-in-python-ab-testing/ http://incompleteideas.net/book/the-book-2nd.html (Chapter 2) https://en.m.wikipedia.org/wiki/Multi-armed_bandit https://www.tensorflow.org/agents/tutorials/intro_bandit "],["discrete-choice-classification-and-tree-based-ensemble-algorithms.html", "Chapter 2 Discrete Choice, Classification, and Tree-Based Ensemble Algorithms 2.1 Introduction 2.2 The Bias-Variance Tradeoff 2.3 Decision Tree 2.4 Split Criterion 2.5 Pruning 2.6 Bagging and Random Forest 2.7 Boosting and AdaBoost 2.8 Gradient Boosting and XGBoost 2.9 Python Implementation with scikit-learn 2.10 Confusion Matrix and other Performance Metrics 2.11 Comparison the Algorithms 2.12 Summary 2.13 References", " Chapter 2 Discrete Choice, Classification, and Tree-Based Ensemble Algorithms 2.1 Introduction Suppose you are the owner of an e-commerce website that sells a musical instrument in 5 sizes: soprano, alto, tenor, bass, and contrabass. You are considering to open up some physical stores. With physical stores, you need to make more careful inventory decisions. Based on your past experience in shipping out your instruments, you are convinced that different communities can have different tastes toward different sizes. And you want to make inventory decisions accordingly. If the stake for a musical store is too small, consider the classic discrete choice example: automobiles. Sales of different types of automobiles are different among U.S. states, for example, more trucks are sold in Texas than most other states. In such circumstance, answering the question of “what” is more important than “how many”, as most consumers buy a single unit of the item. This scenario is known as “discrete choice” in economics and social sciences. Being able to make a good prediction on such question can inform many business decisions, not only inventory, since the aggregation of individual purchases tells us about the overall demand of a product in a market. In economics and social sciences, the popular approaches to model discrete choice are “top-down”: it starts with an understanding of the data-generating process and some assumptions. Logit and Probit are two widely used models in economics. If the error term is believed to follow a logistic distribution, then use the logit model or logistic regression. If the error term is believed to follow a normal distribution, then use the probit model. If there is nested structure, then use nested-logit. And so on. This is fine, since the focus of economics and social sciences is hypothesis testing and to understand mechanisms. On the contrary, the machine learning approach is more bottom-up: it mostly cares about making good predictions. In other words, we can say that the economics “top-down” approach of discrete choice modeling cares much more about “bias” whereas the machine learning approach considers the bias-variance tradeoff more holistically. 2.2 The Bias-Variance Tradeoff Let’s begin with Variance. A model with a high variance is sensitive to the training data and can capture the fine details in the training data. However, such model is usually difficult to generalize. On the one hand, the test data, or the data that the model is actually applied to, may lack the fine detail presented in the training data. On the other hand, those fine details may not be as important in the actual data as compared to the training data. A model that can capture fine details is almost guaranteed to have low bias. A model with low bias is one that explains the known, or training, data well. In order to predict, we need our machine learning model to learn from known data. A model with high bias can rarely predict well. While models with low bias and low variance do exist, they are rare. Since a model with high bias rarely works well, lowering bias is often considered the first-order task. One way to do so is using models, or specifying hyperparameters of a model, such that more fine details in the data are captured. By so doing, higher variance is introduced. And hence the trade-off. Consider the following example: a zoo wants to build a machine learning algorithm to detect penguin species and deploy it on their smart phone application. Let’s say all that the zoo and the users care about is to tell apart King, Magellanic, and Macaroni penguins. The zoo’s staffs and data scientists took hundreds of photos of penguins in their aquarium, split the penguins into training and test datasets as how tasks like this are usually performed, and built the machine learning model. In their test, with photos that they have set aside earlier, they find that the algorithm is able to identify the penguins correctly 98% of the time. They were happy with the result and so deployed it. However, when users use the application to identify penguins in other zoos, the algorithm fails miserably. Why? It turns out that the machine learning algorithm was not learning to identify penguins by their different features such as head, neck, and tail. Instead, the algorithm identifies the different species of penguins by the tag on their wings: blue is King penguin, red Magellanic, and yellow Macaroni. These are the colors used by the zoo that developed the algorithm, but other zoos have different tags. As a result, this algorithm, which has low bias but high variance due to its dependency on penguins in a single aquarium, is unable to predict or detect the species of penguins outside of the training data. As we will see next, tree-based algorithms are extremely prone to high variance, or over-fitting. 2.3 Decision Tree Let’s first talk about the basic decision tree algorithm. Because we will be using scikit-learn for Python implementation in this chapter, I am using notations and languages similar to that in scikit-learn’s documentation. It should be mentioned at the outset that this is not a comprehensive lecture on the decision tree algorithm. You can easily find more in-depth discussions in books such as An Introduction to Statistical Learning and The Elements of Statistical Learning, or other online learning sources. The focus in this session is aspects of the decision tree algorithm that matter the most for the understanding of the ensemble methods and their applications in economics and business. The simplest way to think about a decision tree algorithm is to consider a flow-chart, especially one that is for diagnostic purposes. Instead of someone building a flow-chart from intuition or experience, we feed data into the computer and the decision tree algorithm would build a flow-chart to explain the data. For example, if we know some characteristics of the music store’s past consumers, and want to know who is more likely to buy the soprano size instruments, a flow-chart built by the decision tree algorithm may look like this: * Is the customer under 30? * Yes: is the customer female? * Yes: has the customer played any instrument before? * Yes: the customer has a 90% chance of buying a soprano instrument * No: the customer has 15% chance of buying a soprano instrument * No: is the customer married? * Yes: the customer has a 5% chance of buying a soprano instrument * No: the customer has 92% chance of buying a soprano instrument * No: is the customer under 50? * Yes: the customer has a 10% chance of buying a soprano instrument * No: has the customer played any instrument before? * Yes: the customer has a 100% chance of buying a soprano instrument * No: the customer has a 20% chance of buying a soprano instrument You can see several basic elements of a decision tree algorithm from the above example: As expected, the tree algorithm resulted in a hierarchical structure that can easily be represented by a tree diagram; The tree structure does not need to be symmetrical. For example, when the answer to “is the customer under 50” is a “yes”, the branch stopped, resulted in a shorter branch compared to the rest of the tree; You may use the same feature more than once. In this example, the question “has the customer played any instrument before” has appeared twice. Also there are two splits based on two different age cutoffs; You can use both categorical and numerical features. In this example, age is numerical, whereas all other features are categorical; It is accustomed to split to only two branches at each node. If you want three branches, you can do it at the next node: two branches at the first node, then one or both of the next nodes split into another 2 branches. There are other elements of a decision tree algorithm that you can not observe directly from this example but are very important. We examine these in more details in the following sections. 2.4 Split Criterion At each node, the split must be based on some criterion. The commonly used criteria are Gini impurity and Entropy (or Log-loss). According to the scikit-learn documentation, let \\[p_{mk}=\\frac{1}{n_m}\\sum_{y\\in Q_m}{I(y=k)}\\] denote the proportion of class \\(k\\) observations in node \\(m\\), where \\(Q_m\\) is the data available at node \\(m\\), \\(n_m\\) is the sample size at node \\(m\\), and \\(I(\\cdot)\\) returns 1 when \\(y=k\\) and 0 otherwise. Then, the Gini impurity is calculated as: \\[H(Q_m)=\\sum_{k}{p_{mk}(1-p_{mk})}\\] whereas Entropy is: \\[H(Q_m)=-\\sum_{k}{p_{mk}\\log{(p_{mk})}}\\] At each node \\(m\\), a candidate is defined by the combination of a feature and a threshold. For example, in the above example, for the question “Is the customer under 30,” the feature is age and the threshold is 30. Let \\(\\theta\\) denote a candidate, which splits \\(Q_m\\) into two partitions: \\(Q_m^{\\text{left}}\\) and \\(Q_m^{\\text{right}}\\). Then the quality of a split with \\(\\theta\\) is computed as the weighted average of the criterion function \\(H(Q_m)\\): \\[G(Q_m, \\theta) = \\frac{n_m^{\\text{left}}}{n_m}H(Q_m^{\\text{left}}(\\theta)) + \\frac{n_m^{\\text{right}}}{n_m}H(Q_m^{\\text{right}}(\\theta))\\] The objective of the decision tree algorithm is to find the candidate that minimizes the quality at each \\(m\\): \\[\\theta^{*} = \\underset{\\theta}{\\operatorname{argmin}} \\ G(Q_m, \\theta)\\] It is straightforward to see that, from either the Gini impurity or the Entropy criterion function, the unconstrained minimum of \\(G(Q_m, \\theta)\\) is achieved at \\(p_{mk}=0\\) or \\(p_{mk}=1\\), i.e., when the result of the split consists of a single class. A quick remark before we move on: although there exists a global optimum for the decision tree algorithm where the quality function is minimized for the whole tree, the computation finding it is too complex. In practice, decision tree algorithms use local optima at each node as described above. 2.5 Pruning If achieving a “pure” branch, where only observations from a single class remained after a split, minimizes the quality function \\(G(Q_m, \\theta)\\), then why did we not achieve that “pure” state in the music store example earlier? There are two main reasons. First, we may not have enough features. Imagine you have two individuals in your data set, one bought a soprano and the other bought a contrabass. These two individuals are almost identical with the only difference being their eye colors. If “eye color” is not one of the features captured in your data set, you will have no way to distinguish these two individuals. On the other hand, imagine we know everything about each and every individual, then it is guaranteed that you can find a “perfect” tree, such that there is a single class of individuals at each end node. Such “perfect” tree may not be unique. At the extreme, imagine a tree such that each end node represents a single individual. The second reason is related to the Bias-Variance Tradeoff. Because the ultimate goal is to predict, fitting a “perfect” tree can result in too high of a variance. Continued with the previous example, your ability to build a perfect tree depends entirely on whether you have “eye color” as a feature in your data set. That means that your algorithm is too sensitive to one particular feature, and if this feature does not exist, your algorithm would fail to build a “perfect” tree (assuming that was the goal). Or, if this feature is somehow absent or incorrectly coded in the data set you are predicting on, your algorithm may break down. This is why a decision tree needs to be pruned. In practice, pruning is often done by specifying two hyperparameters: the maximum depth of the tree (max_depth in scikit-learn) and the minimum number of samples required to split (min_samples_split in scikit-learn). Without going into the technical details, we can intuitively understand that both of these restrictions prevent us from splitting the tree to the extreme case such that each end node represents an individual. In other words, they restrict the growth of a tree. The caveat of a single decision tree algorithm is obvious: it can easily suffer from either high bias or high variance, especially the latter. This is why ensemble methods such as bagging and boosting were introduced. In practice, a single decision tree is rarely used as the “final” model. It is often only used as a demonstrative example. 2.6 Bagging and Random Forest Bagging is one of two ensemble methods based on the decision tree algorithm. Bagging is short for bootstrap aggregation, which explains what bagging algorithms do: select random subsets from the training data set, fit the decision tree algorithm on each subset, and aggregate to get the prediction. There are several variations of Bagging algorithms depending on how random samples are drawn: When random subsets were drawn with replacement (bootstrap), the algorithm is known as Bagging (Breiman, 1996) When random subsets were drawn without replacement, the algorithm is known as Pasting (Breiman, 1999) When random subsets are drawn based on features rather than individuals, the algorithm is known as Subspaces (Ho, 1998) When random subsets are drawn based on both features and individuals, the algorithm is known as Random Patches (Louppe and Geurts, 2012) When random subsets were drawn with replacement (bootstrap) and at each split, a random subset of features is chosen, the algorithm is known as Random Forest (Breiman, 2001) In scikit-learn, the first four algorithms are implemented in BaggingClassifier whereas Random Forest is implemented in RandomForestClassifier. In bagging algorithms, the “aggregation” of results during prediction is usually taken by votes. For example, suppose you have fit your data with the Random Forest algorithm with 1,000 trees, and now you want to know what size of the instrument a new customer is likely to buy. When the algorithm considers the first split, it will look at all 1,000 trees and see which candidate was used the most often. Suppose “Is the customer under 30” appeared in 800 of the trees, then the algorithm would split according to age=30. And so, at each split, the algorithm would take a tally from the 1,000 individual trees and act accordingly, just like how one would look at a flow-chart to determine actions. While a Bagging algorithm helps to reduce bias, the main benefit of bootstrapping is to reduce variance. The Random Forest algorithm, for example, is able to reduce variance in two ways: First, bootstrapping random samples is equivalent to consider many different scenarios. Not only does this mean that the algorithm is less reliant on a particular scenario (the whole training data set), it also makes it possible that one or some of the random scenarios may be similar to the “future,” i.e., the environment that the algorithm needs to make predictions on. Second, by considering a random set of features at each split, the algorithm is less reliant on certain features, and is hence resilient to “future” cases where certain features may be missing or have errors. 2.7 Boosting and AdaBoost While the main benefit of Bagging is in reducing variance, the main benefit of Boosting is to reduce bias and maintain a reasonably low variance. Boosting is able to maintain a low variance because, like Bagging, it also fits many trees. Unlike Bagging, which builds the trees in parallel, Boosting builds them sequentially. The basic idea of boosting is to have incremental (small/“weak”) improvements from one stage to another, which is why the learning algorithms are built sequentially. This idea can be applied to all types of algorithms. In the context of decision tree, a boosting algorithm can be demonstrated by the following pseudocode: Step 1: Build a simple decision tree (weak learner) Step 2: Loop: Minimize weighted error with a tree with a small improvement Stop when reaching a stopping rule Currently, there are three popular types of tree-based boosting algorithms: AdaBoost, Gradient Boosting, and XGBoost. The different algorithms are different in how they boost, i.e., how to implement Step 2. AdaBoost was introduced by Freund and Schapire (1995). It is short for Adaptive Boosting. AdaBoost implements boosting by changing the weights of observations. That is, by making some observations/individuals more important than others. In a training data set with \\(N\\) individuals, the algorithm begins by weighting each individual the same: at a weight of \\(1/N\\). Then it fits a simple decision tree model and makes predictions. Inevitably, it makes better decision for some individuals than others. The algorithm then increases the weight for individuals that it did not make correct/good predictions on in the previous stage/model. This effectively asks the next decision tree algorithm to focus more on these individuals that it has failed to understand previously. And this process continues until a stopping rule is reached. A stopping rule may be, for example, “stops when 98% of the individuals are correctly predicted”. It is straightforward to see that a boosting algorithm lowers bias. But is it often able to main a low variance too? Boosting is able to do so because the tree built at each stage/iteration is different. When making predictions, it takes a weighted average of the models. Some mathematical details are helpful. Let \\(w_{ij}\\) denote the weight of individual \\(i\\) in stage/iteration \\(j\\). In the beginning, i.e., \\(j=1\\), we have \\(w_{i1}=1/N\\) for all \\(i\\) where \\(N\\) is the the total number of individuals in the data set. After the first weak tree is built, we can calculate the error/misclassification rate of stage \\(j\\) as \\[e_j = \\frac{\\sum_{N}{w_{ij}\\times I_{ij}(\\text{incorrect})}}{\\sum_{N}{w_{ij}}}\\] where \\(I_{ij}(\\text{incorrect})\\) equals 1 if the prediction for individual \\(i\\) is incorrect in stage \\(j\\) and 0 otherwise. We can then calculate the stage value of model \\(j\\) with: \\[v_j = \\frac{1}{2}\\log\\left(\\frac{1-e_j}{e_j}\\right)\\] The stage value is used both in updating \\(w_{ij+1}\\), i.e., the weight of individual \\(i\\) in the next stage, and to act as the weight of model \\(j\\) when prediction is computed. To update the weight for the next stage/model, we have \\[w_{ij+1} = w_{ij} \\times \\exp{(v_j \\times I_{ij}(\\hat{y}_{ij}=y_i))}\\] where \\(\\hat{y}_{ij}\\) is the prediction for individual \\(i\\) in stage \\(j\\), and \\(y_i\\) is the true label for individual \\(i\\). For binary classification, it is a convention to expression \\(\\hat{y}_{ij}\\) and \\(y_i\\) as 1 and -1, such that the above equation can be simplified into \\[w_{ij+1} = w_{ij} \\times \\exp{(v_j \\times \\hat{y}_{ij}\\times y_i)}\\] At each stage \\(j(&gt;1)\\), the AdaBoost algorithm aims to minimize \\(e_j\\). To compute the overall/final prediction, let \\(\\hat{y}_{ij}\\) denote the prediction of model/stage \\(j\\) for individual \\(i\\), then the predicted value is calculated by: \\[\\hat{y}_{i} = \\sum_{J}{\\hat{y}_{ij} \\times v_j}\\] where \\(J\\) is the total number of stages. 2.8 Gradient Boosting and XGBoost Gradient Boosting (Friedman, 2001) is another approach to boost. Instead of updating the weight after each stage/model, Gradient Boosting aims to minimize a loss function, using methods such as gradient decent. The default loss function in scikit-learn, which is also the most commonly used in practice, is the binomial deviance: \\[L_j = -2\\sum_{N}{y_i\\log{(\\hat{p}_{ij})} + (1-y_i)\\log{(1-\\hat{p}_{ij})}}\\] where \\(N\\) is the number of individuals, \\(y_i\\) is the true label for individual \\(i\\), and \\(\\hat{p}_{ij}\\) is the predicted probability that individual \\(i\\) at stage \\(j\\) having a label of \\(y\\), and is given by the softmax (logistic) function when log-loss is specified: \\[\\hat{p}_{ij} = \\frac{\\exp{(F_j(x_i))}}{1+\\exp{(F_j(x_i))}}\\] where \\(F_j(x_i)\\) is a numerical predicted value for individual \\(i\\) by regressor \\(F_j(x)\\). Here, \\(F_j(x)\\) is the aggregated regressor in stage \\(j\\), which is given by \\[F_j(x) = F_{j-1}(x) + h_j(x)\\] where \\(h_j(x)\\) is the weak learner/regressor at stage \\(j\\) that minimizes \\(L_j\\). Suppose the algorithm runs a total of \\(M\\) stages, we have the final aggregated regressor \\(F_M(x)\\). Substituting \\(F_M(x)\\) into \\(\\hat{p}_{ij}\\) gives the overall prediction of the Gradient Boosting model. Using first-order Taylor approximation, it can be shown that minimizing \\(L_j\\) is approximately equivalent to predicting the negative gradient of the samples, where the negative gradient for individual \\(i\\) is given by \\[-g_i = -\\left[\\frac{\\partial l_{ij-1}}{\\partial F_{j-1}(x_i)}\\right]\\] where \\(l_{ij-1}\\) is the term inside the summation in \\(L_j\\) (but lagged one stage): \\[l_{ij-1} = y_i\\log{(\\hat{p}_{ij-1})} + (1-y_i)\\log{(1-\\hat{p}_{ij-1})}\\] In other words, while the basic decision tree algorithm aims to predict the true classes, usually represented by indicator variables, Gradient Boosting aims to predict a continuous numerical value which is the gradient. This means that, at each stage, Gradient Boosting is a regression problem rather than a classification problem. Predicting the gradient allows the algorithm to utilize many well developed methods for such task, for example, the Nelder-Mead method or grid search. The discussion above focused on binary classification, which requires a single tree to be built in each stage. In multiclass classification, \\(K\\) trees would be built for \\(K\\) classes. For example, if Gradient Boosting is used to identify the 26 English alphabets, 26 trees are built and fitted in each stage. XGBoost was introduced by Tianqi Chen in 2014. It is short for “eXtreme Gradient Boosting”. Instead of gradient decent, XGBoost implements the Newton’s Method, which is computationally much more demanding than gradient decent and requires a second-order Taylor approximation (instead of only the first-order approximation as in Gradient Boosting). Due to this, in addition to Gradients, XGBoost also calculates the Hessians, which are a set of second-order derivatives (whereas gradients are the first-order derivatives). The Python library xgboost implements XGBoost and can easily be integrated with scikit-learn, which is the library we use to implement all algorithms covered in this chapter. 2.9 Python Implementation with scikit-learn As we have done in other chapters, we will first generate a data set, then fit the data with various algorithms. After we have fitted the models, we will print out some basic performance metrics, chief among them the confusion matrix, and conduct a cross validation exercise. The algorithms we consider include: * Logistic regression * Decision tree classifier * Random forest classifier * Adaboost classifier * Gradient boosting classifier * XGBoost Even though logistic regression is not covered in this chapter, I included it in the Python implementation for comparison purposes. Although not necessary, I also use a Python class in this implementation. Here is the full script: import random import string import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay import xgboost N_GROUP = 5 N_IND = 50000 N_FEATURES = 10 class TreeModels: def __init__( self, n_group: int = 5, n_individuals: int = 10000, n_num_features: int = 10, numeric_only: bool = False, ): &quot;&quot;&quot; Initialize the TreeModels class. Args: n_group (int): Number of groups. Default is 5. n_individuals (int): Number of individuals. Default is 10000. n_num_features (int): Number of numerical features. Default is 10. numeric_only (bool): Flag to indicate whether to use only numerical features. Default is False. Returns: None &quot;&quot;&quot; print(f&#39;There are {n_individuals} individuals.&#39;) print(f&#39;There are {n_group} choices.&#39;) print(f&#39;There are {n_num_features} numerical features and 1 categorical feature.&#39;) self.numeric_only = numeric_only # Generate random numerical features and categorical feature self.num_features = np.random.rand(n_individuals, n_num_features + 2) cat_list = random.choices(string.ascii_uppercase, k=6) self.cat_features = np.random.choice(cat_list, size=(n_individuals, 1)) # Create a DataFrame with numerical features and one-hot encoded categorical feature self.df = pd.DataFrame(self.num_features[:, :-2]) self.df[&#39;cat_features&#39;] = self.cat_features self.df = pd.get_dummies(self.df, prefix=[&#39;cat&#39;]) self.df.columns = self.df.columns.astype(str) if numeric_only: # Cluster the data based on numerical features only # Logistic regression performs the best in this condition kmeans = KMeans(n_clusters=n_group, n_init=&quot;auto&quot;).fit(self.num_features) self.df[&#39;target&#39;] = kmeans.labels_ else: # Cluster the data based on both numerical and categorical features cat_columns = self.df.filter(like=&#39;cat&#39;) kmeans1 = KMeans(n_clusters=n_group, n_init=&quot;auto&quot;).fit(cat_columns) kmeans2 = KMeans(n_clusters=n_group, n_init=&quot;auto&quot;).fit(self.num_features) self.df[&#39;target&#39;] = np.floor((kmeans1.labels_ + kmeans2.labels_) / 2) # Add some random noise to the numerical features numerical_columns = [str(i) for i in range(n_num_features)] for column in numerical_columns: self.df[column] = self.df[column] + random.gauss(mu=0, sigma=3) # Split the data into training and testing sets self.X = self.df.drop(columns=[&#39;target&#39;]) self.y = self.df[&#39;target&#39;] self.X_train, self.X_test, self.y_train, self.y_test = train_test_split( self.X, self.y, test_size=0.3, random_state=42) # Initialize the y_pred variable self.y_pred = np.empty([n_individuals, 1]) # Initialize a dictionary to save results self.results = dict() def show_results(self, clf, clf_name, print_flag=False, plot_flag=True): &quot;&quot;&quot; Train and evaluate a classifier. Args: clf: Classifier object. clf_name (str): Name of the classifier. print_flag (bool): Whether to print results. Default is False. plot_flag (bool): Whether to draw CM plots and save them. Default is True. Returns: None &quot;&quot;&quot; print(clf_name) clf.fit(self.X_train, self.y_train) self.y_pred = clf.predict(self.X_test) # Calculate evaluation metrics train_acc = clf.score(self.X_train, self.y_train) acc = accuracy_score(self.y_test, self.y_pred) precision = precision_score(self.y_test, self.y_pred, average=&#39;weighted&#39;) recall = recall_score(self.y_test, self.y_pred, average=&#39;weighted&#39;) f1 = f1_score(self.y_test, self.y_pred, average=&#39;weighted&#39;) # Perform cross-validation and print the average score cv_score = cross_val_score(clf, self.X, self.y, cv=10) if print_flag: if isinstance(clf, LogisticRegression): print(f&#39;Coefficients: {clf.coef_}&#39;) else: print(f&#39;Feature Importance: {clf.feature_importances_}&#39;) print(f&#39;Training accuracy: {train_acc:.4f}&#39;) print(f&#39;Test accuracy: {acc:.4f}&#39;) print(f&#39;Test precision: {precision:.4f}&#39;) print(f&#39;Test recall: {recall:.4f}&#39;) print(f&#39;Test F1 score: {f1:.4f}&#39;) print(f&#39;Average Cross Validation: {np.mean(cv_score)}&#39;) if plot_flag: # Plot the confusion matrix cm = confusion_matrix(self.y_test, self.y_pred, labels=clf.classes_) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_) disp.plot() plt.savefig(f&quot;cm_{clf_name}_{self.numeric_only}.png&quot;, dpi=150) plt.show() # Save results in self.result dictionary self.results[clf_name] = { &#39;train_acc&#39;: train_acc, &#39;acc&#39;: acc, &#39;precision&#39;: precision, &#39;recall&#39;: recall, &#39;f1_score&#39;: f1, &#39;cv_score&#39;: np.mean(cv_score) } def run_tree_ensembles( n_group: int = 5, n_num_features: int = 10, print_flag: bool = True, plot_flag: bool = True, numeric_only_bool: list = (False, True), n_individuals: int = 50000, ) -&gt; dict: for i in numeric_only_bool: tree = TreeModels(n_group, n_individuals, n_num_features, numeric_only=i) logit = LogisticRegression(max_iter=10000) tree.show_results(logit, &#39;logit&#39;, print_flag, plot_flag) d_tree = DecisionTreeClassifier() tree.show_results(d_tree, &#39;decisiontree&#39;, print_flag, plot_flag) rf = RandomForestClassifier() tree.show_results(rf, &#39;randomforest&#39;, print_flag, plot_flag) ada = AdaBoostClassifier() tree.show_results(ada, &#39;adaboost&#39;, print_flag, plot_flag) gbm = GradientBoostingClassifier() tree.show_results(gbm, &#39;gbm&#39;, print_flag, plot_flag) xgb = xgboost.XGBClassifier() tree.show_results(xgb, &#39;xgboost&#39;, print_flag, plot_flag) return {n_individuals: tree.results} Here are some remarks about the script. First, the number of numerical features in the generated data set is given by n_num_features. Two additional columns of numerical features and six columns of string/categorical features are also included to add randomness and complexity to the generated data. The numerical features are stored in the numpy array num_features while the categorical features are stored in cat_features. These features are then properly processed and stored in the pandas dataframe df: Only the original numerical feature columns are stored (self.num_features[:, :-2]); The categorical features are one-hot encoded with pd.get_dummies(). In the if statement that followed, the KMeans algorithm is called to generate n_group classes/clusters. Additional randomness is added to the numerical features by the following: # Add some random noise to the numerical features numerical_columns = [str(i) for i in range(n_num_features)] for column in numerical_columns: self.df[column] = self.df[column] + random.gauss(mu=0, sigma=3) The rest of the TreeModels class performs the train-test split and adds a method named show_results() to run the selected algorithm followed by printing out (based on the value of print_flag) several performance metrics. 2.10 Confusion Matrix and other Performance Metrics Confusion matrix is the most important and common way to examine the performance of a classification algorithm. It is a matrix showing the numbers of individuals in each true-predicted label combination. In our simulated data, there are 5 classes, which results in a 5-by-5 confusion matrix. Below is the confusion matrix of the test data from the logistic regression when the simulation has included categorical features in generating the target groups: Comparison In the confusion matrix, the rows show the “True label” whereas the columns show the “Predicted label”. All the cells on the diagonal are corrected predicted. Based on the confusion matrix, there are three basic performance metrics: accuracy, precision, and recall. There are also various metrics that are weighted averages. For example, the f1 score is the harmonic mean of precision and recall. Accuracy is the proportion of individuals that the algorithm has predicted correctly. To calculate the accuracy score, we sum up the values on the diagonal then divide the total number of individuals in the data set. From the above example, we can calculate accuracy by: \\[\\frac{1884+2769+2945+1553+404}{15000}=0.6370\\] Precision and recall are usually defined based on a certain class. For overall precision and recall scores, we can take a weighted average. Precision is the proportion of individuals who the algorithm predicted to be a certain class is actually that class. In the above example, 2577 individuals were predicted to be class 0, but only 1884 actually are. As a result, the precision for class 0 is: \\[\\frac{1884}{2577}=0.7311\\] Recall, On the other hand, is the proportion of individuals who belong to a certain class that the algorithm predicted correctly. In the above example, 4979 individuals belong to class 2, but only 2945 were predicted correctly by the algorithm. As a result, the recall for class 2 is: \\[\\frac{2945}{4979}=0.5915\\] If we take weighted average of precision and recall of all 5 classes, we get the overall precision and recall scores as 0.6376 and 0.6370, or about 63.7%. These are the values reported as precision_score and recall_score by scikit-learn. Economics and social sciences often use the terms “Type I” and “Type II” errors, which can be related to the discussion here in case of binary classification. In a binary classification, we have 4 quadrants: 1. True positive (TP): those who belong to the “positive” class and are predicted so; 2. True negative (TN): those who belong to the “negative” class and are predicted so. True positive and true negative are on the diagonal; 3. False positive (FP): those who are predicted to be “positive” but are actually “negative’; 4. False negative (FN): those who are predicted to be”negative” but are actually “positive”. Type I error corresponds to false positive and Type II error corresponds to false negative. Before we move on to formally compare results from the 6 algorithms, it is worth noting that random forest, gradient boosting, and XGBoost performed much better than logistic regression in the above simulated data set with categorical features and random seed = 123. For example, below is the confusion matrix from XGBoost: Comparison 2.11 Comparison the Algorithms The following Python script runs the comparison between different algorithms for between 6000 and 50000 individuals (sample size): import matplotlib.pyplot as plt import random import pandas as pd from multiprocessing import Pool, cpu_count from functools import partial from tree_ensembles import run_tree_ensembles plt.ion() n_individuals_range = range(50000, 5999, -2000) def run_monte_carlo(n_individuals_range, numeric_only_bool): with Pool(1) as pool: func = partial(run_tree_ensembles, 5, 10, False, False, numeric_only_bool) results = list(pool.imap(func, n_individuals_range)) return results def plot_monte_carlo(data: list): df_list = [] for item in data: for i, inner_dict in item.items(): for j, inner_inner_dict in inner_dict.items(): value = inner_inner_dict[&#39;cv_score&#39;] df_list.append({&#39;i&#39;: i, &#39;Model&#39;: j, &#39;cv_score&#39;: value}) df = pd.DataFrame(df_list) fig, ax = plt.subplots() num_models = len(df[&#39;Model&#39;].unique()) cmap = plt.get_cmap(&#39;Set2&#39;) # Use the Set2 color map for i, model in enumerate(df[&#39;Model&#39;].unique()): model_data = df[df[&#39;Model&#39;] == model] color = cmap(i % num_models) # Cycle through the color map ax.plot(model_data[&#39;i&#39;], model_data[&#39;cv_score&#39;], &#39;-o&#39;, c=color, label=model, alpha=0.5) ax.set_xlabel(&#39;Number of Individuals&#39;) ax.set_ylabel(&#39;Cross Validation Scores&#39;) ax.set_title(&#39;Plot of Cross Validation Scores&#39;) ax.legend([&#39;Logit&#39;, &#39;Decision Tree&#39;, &#39;Random Forest&#39;, &#39;Adaboost&#39;, &#39;GBM&#39;, &#39;XGBoost&#39;], loc=&#39;lower right&#39;, fontsize=9, markerscale=1.5, scatterpoints=1, fancybox=True, framealpha=0.5) The script intends to use parallel computing, but algorithms in scikit-learn already utilized parallel computing, so Pool(1) is set to run with a single thread. Two comparisons, with and without using the categorical features in generating the target groups, were run. The average score from 10-fold cross validations are recorded and plotted. Here is the result from when KMeans generated the target groups without using the categorical columns (but they are still present in the training data and used as features): Comparison A single decision tree performed the worst, while, surprisingly, logistic regression performed the best. Keep in mind that the ups and downs at different sample sizes do not indicate that more data is worse. There is some random components in how the data was generated with the KMeans algorithm. When categorical columns are included in generating the target groups, there exists more variations among algorithms: Comparison In here, Adaboost performed noticeably worse than all other algorithms. Logistic regression also fell, partly because its inability to deal with categorical features (even with one-hot encoding). Not surprisingly, the performances of Random Forest, Gradient Boosting, and XGBoost remain strong. 2.12 Summary In this chapter, we have covered the decision tree algorithm as well as bagging and boosting algorithms based on decision tree. Here are a few important takeaways and remarks. First, ensemble methods is a general method that applies to algorithms beyond tree-based models. You could apply the same principle of bagging and boosting on regression models. For example, you can build several regressors with a bootstrap data set, or include only some of the features, or use weighted methods to boost. As a matter of fact, ensemble can also be built between regression and classification algorithms. Gradient Boosting and XGBoost are already such ensembles: regression models were used in in each stage for an eventual prediction of classes. Second, tree-based models can be used for regression problems. For example, instead of RandomForestClassifier, you can use RandomForestRegressor from scikit-learn to implement a Random Forest algorithm regression problems. When you are using a classification algorithm on a continuous target, the algorithm aims to predict the mean of the target instead of trying to predict classes. We will cover this in more depth in a later chapter. Third, in general, it is more accurate to predict classes than continuous values. Due to this, the use of classification algorithms may be broader than most would think. For example, it is possible to convert a regression problem (in predicting continuous quantities) to classification problems. The market share example given in the beginning of the chapter is a good example. Another example is e-commerce. Most e-commerce owners have a limited offering. As a result, instead of predicting the total sales per month or the dollar value of a customer, it is easier to predict whether and how many units a customer would buy. This method can be especially powerful since a business owner often has control over the price of the products. Lastly, tree-based methods can be used for causal inference. While causal inference itself is a topic of a later chapter, for readers who are familiar with causal inference methods, you can easily find parallel between decision tree and propensity score matching (PSM): individuals who ended up in the same leave/node have something in common, and hence can be considered as good matches. This is the basic idea behind causal tree (Athey and Imbens, 2016). 2.13 References S. Athey and G. Imbens, “Recursive partitioning for heterogeneous causal effects”, PNAS, 2016. L. Breiman, “Bagging predictors”, Machine Learning, 1996. L. Breiman, “Pasting small votes for classification in large databases and on-line”, Machine Learning, 1999. L. Breiman, “Random forest”, Machine Learning, 2001. Y. Freund and R. Schapire, “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting”, 1995. J. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine”, The Annals of Statistics, 2001. T. Ho, “The random subspace method for constructing decision forests”, Pattern Analysis and Machine Intelligence, 1998. G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine Learning and Knowledge Discovery in Databases, 2012. https://scikit-learn.org/stable/modules/tree.html https://xgboost.readthedocs.io/en/stable/tutorials/model.html https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/ https://stats.stackexchange.com/questions/157870/scikit-binomial-deviance-loss-function https://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/ "],["time-series-forecasting-and-deep-learning-algorithms.html", "Chapter 3 Time Series, Forecasting, and Deep Learning Algorithms 3.1 Introduction 3.2 Time Series Implementation in statsmodels 3.3 Artificial Neural Network (ANN) 3.4 ANN in TensorFlow/Keras 3.5 Recurrent Neural Network (RNN) 3.6 RNN in TensorFlow/Keras 3.7 Convolutional Neural Network (CNN) 3.8 CNN in TensorFlow/Keras 3.9 Facebook Prophet 3.10 Summary 3.11 References", " Chapter 3 Time Series, Forecasting, and Deep Learning Algorithms 3.1 Introduction This chapter is structured differently from other chapters. We will begin with Python implementations for time-series/forecasting models that are not based on machine learning. This is accomplished primarily with the Python library statsmodels. The section serves as both a review of forecasting concepts and an introduction to the statsmodels library, another widely used Python library for statistical/data analysis. The main emphasis of this chapter, however, is the use of deep learning models for forecasting tasks. We will introduce three neural network models: Artificial Neural Networks (ANN), Reccurent Neural Networks (RNN), and Convolutional Neural Networks (CNN). We will implement these models in Python using Keras from TensorFlow. The chapter ends with the introduction toFacebook'sProphet` library, which is a widely-used library for forecasting in the industry. Forecasting should need no introduction. At its simplest form, you have a time series data set with values of a single object/individual overtime, and you attempt to predict the “next” value into the future. In more complicated cases, you may have covariates/features, as long as these features are observable at the moment of forecasting and do not result in information leakage. For example, if you are doing weather forecast and your goal is to forecast whether it is going to rain tomorrow, your data set should contain only information of whether it has rained or not in the past many days in which additional features such as temperature, dew point, and precipitation may be included. These additional weather variables should be from the day before your forecast, not the day of your forecast, when you are training your model. A classic example of information leakage happens when forecasting with moving average (MA) values. For example, if you are doing a 3-day MA, then the value of today requires the use of the value from tomorrow, which is only possible in historic data but not with real data. 3.2 Time Series Implementation in statsmodels In this section, we will implement three forecasting models: Exponential Smoothing (ETS), Vector Autoregression (VAR), and Autoregressive Integrated Moving Average (ARIMA). ETS and ARIMA are run with a single time series, whereas VAR uses several. The data set we will use is U.S. stock exchange (close) prices from the Python library yfinance. For ETS, we will also implement a walk-forward validation, which is the correct form of validation for time series data, analogue to cross validation seen in the last chapters. To show the power of Auto Machine Learning, we will implement auto ARIMA from the Python library pmdarima. Here is the full Python script: import yfinance as yf import pandas as pd import matplotlib.pyplot as plt import numpy as np import itertools from statsmodels.graphics.tsaplots import plot_acf, plot_pacf from statsmodels.tsa.holtwinters import ExponentialSmoothing from statsmodels.tsa.api import VAR import pmdarima as pm from sklearn.metrics import mean_squared_error, r2_score from sklearn.preprocessing import StandardScaler import warnings warnings.filterwarnings(&quot;ignore&quot;) # ignore warnings def prepare_data(df): &quot;&quot;&quot; Split the data into training and testing sets. Args: df (pandas.DataFrame): The input dataframe. Returns: tuple: A tuple containing the train set, test set, train index, and test index. &quot;&quot;&quot; train = df.iloc[:-N_TEST] test = df.iloc[-N_TEST:] train_idx = df.index &lt;= train.index[-1] test_idx = df.index &gt; train.index[-1] return train, test, train_idx, test_idx def plot_fitted_forecast(df, col=None): &quot;&quot;&quot; Plot the fitted and forecasted values of a time series. Args: df (pandas.DataFrame): The input dataframe. col (str): The column name to plot. Default is None. &quot;&quot;&quot; df = df[-108:] # only plot the last 108 days fig, ax = plt.subplots(figsize=(15, 5)) ax.plot(df.index, df[f&quot;{col}&quot;], label=&#39;data&#39;) ax.plot(df.index, df[&#39;fitted&#39;], label=&#39;fitted&#39;) ax.plot(df.index, df[&#39;forecast&#39;], label=&#39;forecast&#39;) plt.legend() plt.show() class StocksForecast: def __init__(self, stock_name_list=(&#39;UAL&#39;, &#39;WMT&#39;, &#39;PFE&#39;), start_date=&#39;2018-01-01&#39;, end_date=&#39;2022-12-31&#39;): &quot;&quot;&quot; Initialize the StocksForecast class. Args: stock_name_list (list[str]): List of stock names. Default is (&#39;UAL&#39;, &#39;WMT&#39;, &#39;PFE&#39;). start_date (str): Start date of the data. Default is &#39;2018-01-01&#39;. end_date (str): End date of the data. Default is &#39;2022-12-31&#39;. &quot;&quot;&quot; self.dfs = dict() for name in stock_name_list: self.dfs[name] = yf.download(name, start=start_date, end=end_date) self.dfs[name][&#39;Diff&#39;] = self.dfs[name][&#39;Close&#39;].diff(1) self.dfs[name][&#39;Log&#39;] = np.log(self.dfs[name][&#39;Close&#39;]) def run_ets(self, stock_name=&#39;UAL&#39;, col=&#39;Close&#39;): &quot;&quot;&quot; Run the Exponential Smoothing (ETS) model on the specified stock. Args: stock_name (str): The name of the stock. Default is &#39;UAL&#39;. col (str): The column name to use for the model. Default is &#39;Close&#39;. &quot;&quot;&quot; df_all = self.dfs[stock_name] train, test, train_idx, test_idx = prepare_data(df_all) model = ExponentialSmoothing(train[col].dropna(), trend=&#39;mul&#39;, seasonal=&#39;mul&#39;, seasonal_periods=252) result = model.fit() df_all.loc[train_idx, &#39;fitted&#39;] = result.fittedvalues df_all.loc[test_idx, &#39;forecast&#39;] = np.array(result.forecast(N_TEST)) plot_fitted_forecast(df_all, col) def walkforward_ets(self, h, steps, tuple_of_option_lists, stock_name=&#39;UAL&#39;, col=&#39;Close&#39;, debug=False): &quot;&quot;&quot; Perform walk-forward validation on the specified stock. Only supports ExponentialSmoothing Args: h (int): The forecast horizon. steps (int): The number of steps to walk forward. tuple_of_option_lists (tuple): Tuple of option lists for trend and seasonal types. stock_name (str): The name of the stock. Default is &#39;UAL&#39;. col (str): The column name to use for the model. Default is &#39;Close&#39;. debug (bool): Whether to print debug information. Default is False. Returns: float: The mean of squared errors. &quot;&quot;&quot; errors = [] seen_last = False steps_completed = 0 df = self.dfs[stock_name] Ntest = len(df) - h - steps + 1 trend_type, seasonal_type = tuple_of_option_lists for end_of_train in range(Ntest, len(df) - h + 1): train = df.iloc[:end_of_train] test = df.iloc[end_of_train:end_of_train + h] if test.index[-1] == df.index[-1]: seen_last = True steps_completed += 1 hw = ExponentialSmoothing(train[col], trend=trend_type, seasonal=seasonal_type, seasonal_periods=40) result_hw = hw.fit() forecast = result_hw.forecast(h) error = mean_squared_error(test[col], np.array(forecast)) errors.append(error) if debug: print(&quot;seen_last:&quot;, seen_last) print(&quot;steps completed:&quot;, steps_completed) return np.mean(errors) def run_walkforward(self, h, steps, stock_name, col, options): &quot;&quot;&quot; Perform walk-forward validation on the specified stock using Exponential Smoothing (ETS). Args: h (int): The forecast horizon. steps (int): The number of steps to walk forward. stock_name (str): The name of the stock. col (str): The column name to use for the model. options (tuple): Tuple of option lists for trend and seasonal types. Returns: float: The mean squared error (MSE) of the forecast. &quot;&quot;&quot; best_score = float(&#39;inf&#39;) best_options = None for x in itertools.product(*options): score = self.walkforward_ets(h=h, steps=steps, stock_name=stock_name, col=col, tuple_of_option_lists=x) if score &lt; best_score: print(&quot;Best score so far:&quot;, score) best_score = score best_options = x trend_type, seasonal_type = best_options print(f&quot;best trend type: {trend_type}&quot;) print(f&quot;best seasonal type: {seasonal_type}&quot;) def prepare_data_var(self, stock_list, col): &quot;&quot;&quot; Prepare the data for Vector Autoregression (VAR) modeling. Args: stock_list (list): List of stock names. col (str): The column name to use for the model. Returns: tuple: A tuple containing the combined dataframe, train set, test set, train index, test index, stock columns, and scaled columns. &quot;&quot;&quot; df_all = pd.DataFrame(index=self.dfs[stock_list[0]].index) for stock in stock_list: df_all = df_all.join(self.dfs[stock][col].dropna()) df_all.rename(columns={col: f&quot;{stock}_{col}&quot;}, inplace=True) train, test, train_idx, test_idx = prepare_data(df_all) stock_cols = df_all.columns.values # standardizing different stocks for value in stock_cols: scaler = StandardScaler() train[f&#39;Scaled_{value}&#39;] = scaler.fit_transform(train[[value]]) test[f&#39;Scaled_{value}&#39;] = scaler.transform(test[[value]]) df_all.loc[train_idx, f&#39;Scaled_{value}&#39;] = train[f&#39;Scaled_{value}&#39;] df_all.loc[test_idx, f&#39;Scaled_{value}&#39;] = test[f&#39;Scaled_{value}&#39;] cols = [&#39;Scaled_&#39; + value for value in stock_cols] return df_all, train, test, train_idx, test_idx, stock_cols, cols def run_var(self, stock_list=(&#39;UAL&#39;, &#39;WMT&#39;, &#39;PFE&#39;), col=&#39;Close&#39;): &quot;&quot;&quot; Run the Vector Autoregression (VAR) model on the specified stocks. Args: stock_list (tuple): Tuple of stock names. Default is (&#39;UAL&#39;, &#39;WMT&#39;, &#39;PFE&#39;). col (str): The column name to use for the model. Default is &#39;Close&#39;. &quot;&quot;&quot; df_all, train, test, train_idx, test_idx, stock_cols, cols = self.prepare_data_var(stock_list, col) model = VAR(train[cols]) result = model.fit(maxlags=40, method=&#39;mle&#39;, ic=&#39;aic&#39;) lag_order = result.k_ar prior = train.iloc[-lag_order:][cols].to_numpy() forecast_df = pd.DataFrame(result.forecast(prior, N_TEST), columns=cols) df_all.loc[train_idx, &#39;fitted&#39;] = result.fittedvalues[cols[0]] df_all.loc[test_idx, &#39;forecast&#39;] = forecast_df[cols[0]].values col = &quot;Scaled_&quot; + stock_cols[0] plot_fitted_forecast(df_all, col) # Calculate R2 print(&quot;VAR Train R2: &quot;, r2_score(df_all.loc[train_idx, cols[0]].iloc[lag_order:], df_all.loc[train_idx, &#39;fitted&#39;].iloc[lag_order:])) print(&quot;VAR Test R2: &quot;, r2_score(df_all.loc[test_idx, cols[0]], df_all.loc[test_idx, &#39;forecast&#39;])) def run_arima(self, stock_name=&#39;UAL&#39;, col=&#39;Close&#39;, seasonal=True, m=12): &quot;&quot;&quot; Run the Auto Autoregressive Integrated Moving Average (ARIMA) model on the specified stock. Args: stock_name (str): The name of the stock. Default is &#39;UAL&#39;. col (str): The column name to use for the model. Default is &#39;Close&#39;. seasonal (bool): Whether to include seasonal components. Default is True. m (int): The number of periods in each seasonal cycle. Default is 12. &quot;&quot;&quot; df_all = self.dfs[stock_name] train, test, train_idx, test_idx = prepare_data(df_all) plot_acf(train[col]) plot_pacf(train[col]) model = pm.auto_arima(train[col], trace=True, suppress_warnings=True, seasonal=seasonal, m=m) print(model.summary()) df_all.loc[train_idx, &#39;fitted&#39;] = model.predict_in_sample(end=-1) df_all.loc[test_idx, &#39;forecast&#39;] = np.array(model.predict(n_periods=N_TEST, return_conf_int=False)) plot_fitted_forecast(df_all, col) if __name__ == &quot;__main__&quot;: # parameters STOCK = &#39;UAL&#39; COL = &#39;Log&#39; N_TEST = 10 H = 20 # 4 weeks STEPS = 10 # Hyperparameters to try in ETS walk-forward validation trend_type_list = [&#39;add&#39;, &#39;mul&#39;] seasonal_type_list = [&#39;add&#39;, &#39;mul&#39;] init_method_list = [&#39;estimated&#39;, &#39;heuristic&#39;, &#39;legacy-heristic&#39;] # not used use_boxcox_list = [True, False, 0] # not used ts = StocksForecast() ts.run_ets(stock_name=STOCK, col=COL) ts.run_var(col=COL) ts.run_arima(stock_name=STOCK, col=COL) tuple_of_option_lists = (trend_type_list, seasonal_type_list,) ts.run_walkforward(H, STEPS, STOCK, COL, tuple_of_option_lists) As in other chapters, a class, named StocksForecast, is written. In the beginning of the script, we have two static methods/functions outside of the class for data preparation and plotting. For StockForecast, we initiate the class with: download the data store data into a dictionary with each stock in a different key calculate the log and first-differenced values of close price. def __init__(self, stock_name_list=(&#39;UAL&#39;, &#39;WMT&#39;, &#39;PFE&#39;), start_date=&#39;2018-01-01&#39;, end_date=&#39;2022-12-31&#39;): &quot;&quot;&quot; Initialize the StocksForecast class. Args: stock_name_list (list[str]): List of stock names. Default is (&#39;UAL&#39;, &#39;WMT&#39;, &#39;PFE&#39;). start_date (str): Start date of the data. Default is &#39;2018-01-01&#39;. end_date (str): End date of the data. Default is &#39;2022-12-31&#39;. &quot;&quot;&quot; self.dfs = dict() for name in stock_name_list: self.dfs[name] = yf.download(name, start=start_date, end=end_date) self.dfs[name][&#39;Diff&#39;] = self.dfs[name][&#39;Close&#39;].diff(1) self.dfs[name][&#39;Log&#39;] = np.log(self.dfs[name][&#39;Close&#39;]) Each model is implemented inside a wrapper function. For example, the ETS implementation is in run_ets(), which does the following: call the prepare_data() function instantiate the ExponentialSmoothing model from statsmodels with hyperparameters trend, seasonal, and seasonal_periods. For trend and seasonal, mul means these trends are multiplicative. The value 252 (days) is used for seasonal_periods since this is about the number of trading days in half a year call model.fit() get forecast columns and prepare the data for plotting call the plot_fitted_forecast() function to plot def run_ets(self, stock_name=&#39;UAL&#39;, col=&#39;Close&#39;): &quot;&quot;&quot; Run the Exponential Smoothing (ETS) model on the specified stock. Args: stock_name (str): The name of the stock. Default is &#39;UAL&#39;. col (str): The column name to use for the model. Default is &#39;Close&#39;. &quot;&quot;&quot; df_all = self.dfs[stock_name] train, test, train_idx, test_idx = prepare_data(df_all) model = ExponentialSmoothing(train[col].dropna(), trend=&#39;mul&#39;, seasonal=&#39;mul&#39;, seasonal_periods=252) result = model.fit() df_all.loc[train_idx, &#39;fitted&#39;] = result.fittedvalues df_all.loc[test_idx, &#39;forecast&#39;] = np.array(result.forecast(N_TEST)) plot_fitted_forecast(df_all, col) A walk-forward validation for ETS is implemented by the method run_walkforward() (largely from the Lazy Programmer) which is a wrapper function of warlkforward_ets(). For time series data, we can not perform cross-validation by selecting a random subset of observations, as this can result in using future values to predict past value. Instead, a n-step walk-forward validation should be used. Suppose we have data from 1/1/2018 to 12/31/2022, a 1-step walk-forward validation using data from December 2022 would involve the following steps: train the model with data from 1/1/2018 to 11/30/2022 with model result, make prediction for 12/1/2022 compare the true and predicted values and calculate the error or other desire metric(s) “walk forward” by 1 day, then go back to training the model, i.e., train the model with data from 1/1/2018 to 12/1/2022 continue until data from 1/1/2018 to 12/30/2022 is used for training and 12/31/2022 is predicted We should try several different hyperparameter combinations since the purpose of the walk-forward validation is to choose the “best” hyperparameters. The following lines inside if __name__ == \"__main__\": calls the run_walkforward() method to try a combination of hyperparameters, which also prints out the “best” values for trend and seasonal: H = 20 # 4 weeks STEPS = 10 # Hyperparameters to try in ETS walk-forward validation trend_type_list = [&#39;add&#39;, &#39;mul&#39;] seasonal_type_list = [&#39;add&#39;, &#39;mul&#39;] tuple_of_option_lists = (trend_type_list, seasonal_type_list,) ts.run_walkforward(H, STEPS, STOCK, COL, tuple_of_option_lists) The method run_var() runs the VAR model. Since we run VAR with several stocks, standardized/normalized should be performed. This is accomplished in the prepare_data_var() method with StandardScaler() from scikit-learn. Last but not least, the run_arima() method runs the Auto ARIMA from the pdmarima library. Here, we also call plot_acf() and plot_pacf() from scikit-learn to examine the autocorrelation and partial autocorrelation functions. Normally, they are important for the ARIMA model. However, with Auto ARIMA, we are spared of the task of manually determine the values of AR() and MA(). Similar to run_ets(), there are only a few lines of code: def run_arima(self, stock_name=&#39;UAL&#39;, col=&#39;Close&#39;, seasonal=True, m=12): &quot;&quot;&quot; Run the Auto Autoregressive Integrated Moving Average (ARIMA) model on the specified stock. Args: stock_name (str): The name of the stock. Default is &#39;UAL&#39;. col (str): The column name to use for the model. Default is &#39;Close&#39;. seasonal (bool): Whether to include seasonal components. Default is True. m (int): The number of periods in each seasonal cycle. Default is 12. &quot;&quot;&quot; df_all = self.dfs[stock_name] train, test, train_idx, test_idx = prepare_data(df_all) plot_acf(train[col]) plot_pacf(train[col]) model = pm.auto_arima(train[col], trace=True, suppress_warnings=True, seasonal=seasonal, m=m) print(model.summary()) df_all.loc[train_idx, &#39;fitted&#39;] = model.predict_in_sample(end=-1) df_all.loc[test_idx, &#39;forecast&#39;] = np.array(model.predict(n_periods=N_TEST, return_conf_int=False)) plot_fitted_forecast(df_all, col) If you would like to run ARIMA from statsmodels, you can import ARIMA from statsmodels.tsa.arima.model. statsmodels also provides functions and APIs for other time-series/forecasting methods and models. For example, you can test for stationarity with the augmented Dickey-Fuller unit root test by importing adfuller from statsmodels.tsa.stattools, or run the Vector Autoregressive Moving Average with exogenous regressors by importing VARMAX from statsmodels.tsa.statespace.varmax. In addition, if you would like to do the Box-Cox transformation, you can import boxcox from scipy.stats. 3.3 Artificial Neural Network (ANN) Similar to other chapters, we assume that readers have some idea about what a neural network is and what it can do. Our goal is not to give an in-depth introduction to neural networks. Rather, we will only cover elements of neural networks that matter most in their applications in economics and business assuming readers already have some quantitative training. An excellent place that you can “play” with a neural network model is the Tensorflow Playground. Neural networks can be used on both regression and classification problems. Our focus in this chapter is to use neural networks on regression since the emphasis is forecasting. Keep in mind that we can always reshape a regression problem into a classification problem. For example, instead of forecasting the actual price or return of a stock, we can predict the likelihood of a stock trending up or down, which is a binary classification problem. The difference between applying neural networks on regression or classification problems is minor: for regression problems, the final activation function is an identify function (returns itself) whereas for classification problems it is Sigmoid or other functions that return values between 0 and 1. A really good summary of activation functions is this answer on stackexchange. Let us begin with artificial neural network (ANN). For implementation of neural networks, we are using Keras (https://keras.io/) from Tensorflow (https://www.tensorflow.org/). We will introduce PyTorch, another popular deep learning library, in other chapters. Neural network models intend to mimic the human brain. The basic idea can be described as follow. Imagine you see an ice-cream truck and decide to try an ice-cream that you have not had before. First, you receive multiple signals: you see the brand, shape, color, and possibly smell and ingredients of many ice-creams that you can choose from. These “raw” signals are first passed through the initial layer of neurons, the ones immediately connected to your eyes and noses and other sensory organs. After the initial layer and processing, you recognize different features of many ice-creams, some excites you, some not. In neural science terminology, the outputs from the first layer of neurons have different “action potential”. If the action potential passes a certain threshold, it excites you. But such excitement can be both positive and negative. For example, you may recognize there are peanuts in some of the ice-creams cones. While the crunchy cone excites you, you also know that you are allergic to peanuts. Imagine in the second layer, one neuron specializes in recognizing cones and the other peanuts. The output from the first layer would activate both of these two neurons. And hence the name “activation function”. This process can continue. A neural network may contain many layers, and each layer many neurons. After passing through all the layers, you have arrived at your decision: A cup with vanilla and strawberry ice-creams and chocolate chips on top. Suppose your raw data set has \\(N\\) observations/rows and \\(M\\) features/columns. The probability of the \\(i\\)’s neuron in the first layer being activated is \\[z^{(1)}_i=p(\\text{activated} \\mid x)=\\sigma(xW^{(1)}_i+b^{(1)}_i)\\] where \\(x\\) is a \\(N\\times M\\) matrix, \\(W^{(1)}_i\\) and \\(b^{(1)}_i\\) are both vectors of size \\(M\\), and \\(\\sigma()\\) is an activation function that returns a probability such as Sigmoid or ReLU. In regression terminology, \\(W^{(1)}_i\\) are the coefficients and \\(b^{(1)}_i\\) is the intercept. By neural network convention, we use the superscript \\((j)\\) to denote layer. Usually each layer has multiple neurons. In this case, the outputs \\(z^{(j)}_i\\) can be “stacked” horizontally and fed into the next lay. We an similarly stack \\(W^{(j)}_i\\) and \\(b^{(j)}_i\\). In other words, the number of neurons in the current layer (\\(j\\)) is the number of features for the next layer layer (\\(j+1\\)). With this, we can express the whole neural network in the following manner: Beginning (\\(j=1\\)): \\(z^{(1)}=\\sigma(xW^{(1)}+b^{(1)})\\) Hidden layers (\\(1&lt;j&lt;J\\)): \\(z^{(j)}=\\sigma(z^{(j-1)}W^{(j)}+b^{(j)})\\) Final layer (\\(j=J\\)): \\(\\hat{y}=z^{(L-1)}W^{(L)}+b^{(L)}\\) where \\(J\\) denotes the total number of layers, and \\(\\hat{y}\\) is the prediction. Note that the final layer does not have an activation function here because we are dealing with a regression model. While Sigmoid is a widely used function when probabilities are to be predicted, it suffers from the vanishing gradient problem especially with deep (many layers) neural networks. Modern deep learning models often use ReLU or tanh as the activation function for inner layers. Again, see this answer on stackexchange for the pros and cons of different activation functions in neural networks. 3.4 ANN in TensorFlow/Keras 3.5 Recurrent Neural Network (RNN) There is a compelling reason why Recurrent Neural Network (RNN) models are often expected to perform well in time-series/forecasting tasks: it is the neural network version of the autoregressive (AR) process. in its simplest form, often referred to as Simple RNN, the output from the hidden layers of time \\(t-1\\) is used as inputs for time \\(t\\) in addition to \\(x\\). Suppose you only care about one-step forecast, i.e., you want to predict \\(t+1\\) with data up to time \\(t\\). Suppose we use all data for training, the approaches covered in this chapter so far have basically the same flavor: specify a single model for any length of time, train the model using data up to time \\(t\\), and make the prediction for \\(t+1\\). Even with walk-forward validation, it is not much different except that several values of \\(t\\) are considered and hence the model was trained on different data and can have different parameters dependent on the value of \\(t\\). Having a single unified model is often fine as long as the time series does not have large ups and downs. Unfortunately, economics and business time-series data only consists of ups and downs, such as a recession. In such cases, we often want to specify more than one model. That can be accomplished manually if we know exactly when a structural break has happened. But life is a box of chocolates and every hour/day is different. It would be nice that a model can do the following: that it “remembers” the past and customizes a model for the current time. RNN does exactly that. Concretely, let \\(h_t\\) denote the hidden state of an RNN at time \\(t\\), we have \\[h_t = \\sigma(h_{t-1}W_{ht} + x_tW_{xt}+b_{t})\\] where \\(W_{ht}\\) and \\(W_{xt}\\) are coefficients/weights for the hidden state and input \\(x_t\\), respectively, at time \\(t\\), and \\(b_{t}(= b_{ht} + b_{xt})\\) is the intercept. The hidden state allows the model to “remember” the past and adds non-linear complexity to each time period. It should be noted that \\(h_t\\) can be a mini ANN with many hidden layers. In addition to Simple RNN, Long Short-Term Member (LSTM) and Gated Recurrent Units (GRU) are two widely used RNN models. Both models modified how hidden state is being remembered from one time period (or one state) to another. For GRU, two “gates” are introduced: Update gate: \\(z_t = \\sigma(x_tW_{xzt}+h_{t-1}W_{hzt}+b_{zt})\\) Reset gate: \\(r_t = \\sigma(x_tW_{xrt}+h_{t-1}W_{hrt}+b_{rt})\\) And the hidden state is updated according to \\[h_t = (1-z_t)\\odot h_{t-1} + z_t\\odot \\omega(x_tW_{xht}+(r_t\\odot h_{t-1})W_{hht}+b_{ht})\\] where \\(\\odot\\) is an element-wise multiplication and \\(\\omega()\\) is an activation function similar to \\(\\sigma()\\) except that in Tensorflow the default is tanh instead of Sigmoid for RNN. In the GRU, \\(z_t\\) controls how much the neural network “forgets” and \\(r_t\\) controls how much the neural network “learns” from the previous state. If \\(z_t=0\\), then the neural network forgets about the previous state (since \\(1-z_t=0\\)) and relearn. Keep in mind that the relearn, which is \\(\\omega()\\) still consists of the previous hidden state \\(h_{t-1}\\) unless \\(r_t\\) is also equal to 0. For LSTM, we introduce a new state called cell state in addition to the hidden state. In practice, the cell state is an intermediate value that helps to keep track of the model is not included in calculating the final output. The LSTM has three gates: Forget gate: \\(f_t = \\sigma(x_tW_{xft}+h_{t-1}W_{hft}+b_{ft})\\) Input/Update gate: \\(i_t = \\sigma(x_tW_{xit}+h_{t-1}W_{hit}+b_{it})\\) Output gate: \\(o_t = \\sigma(x_tW_{xot}+h_{t-1}W_{hot}+b_{ot})\\) And the hidden state and cell state (\\(c_t\\)) are updated according to: Cell state: \\(c_t = f_t\\odot c_{t-1} + i_t\\odot \\omega(x_tW_{xct}+h_{t-1}W_{hct}+b_{ct})\\) Hidden state: \\(h_t = o_t\\odot \\psi(c_t)\\) Note that in Tensorflow, the activation function \\(\\omega()\\) and \\(\\psi()\\) can not be specified individually and are both defaulted to tanh. 3.6 RNN in TensorFlow/Keras 3.7 Convolutional Neural Network (CNN) Convolutional Neural Network (CNN) is another deep learning algorithm that we can connect to traditional time-series/forecasting methods easily. Consider a typical ARIMA model, which has three parameters: \\(p\\), \\(q\\), and \\(d\\). These parameters dictates the number of periods in, respectively, autoregressive, moving average, and differencing. An alternative way to look at the ARIMA model is that the original time series data is transformed based on the three parameters. There are other transformations and filters performed on time-series data, for example, Fourier transformation, low-pass filter, Baxter-King filter, to name a few. Exponential smoothing, which we have shown its implementation using statsmodels earlier, is also a filter. Differencing and autoregressive process are also filters. Which brings us to CNN: convolving is applying filters on the data. The technical/mathematical details are less important for time-series data, as CNN is a widely used algorithm in computer vision (CV) and there are more nuances in that area. For our purpose, let us focus on the following aspects of CNN. First, convolution does pattern matching/finding with cross-correlation. Imagine a time-series with length \\(T=10\\): \\[ts = [1, 4, 5, 3, 3, 4, 2, 3, 5, 3]\\] and another vector of length \\(K=3\\): \\[c = [1, 5, 1]\\] When we convolved \\(ts\\) with \\(c\\), we are “sliding” \\(c\\) over \\(ts\\) and at each position, we compute the dot product. For example, when \\(c\\) is overlaid on the first three values of \\(ts\\), we have: \\[[1, 5, 1] \\cdot [1, 4, 5]=(1\\times1)+(5\\times4)+(1\\times5)=26\\] Repeating this process, we get a convolved version of \\(ts\\): \\[tsv = [26, 32, 23, 22, 25, 17, 22, 31]\\] The resulted new vector is of size \\(T-K+1\\), which is the valid mode of covolution. If we want the resulted vector to be the same size as the original, we are performing a same mode convolution and we need to add padding of size \\(K-1\\). In our example, we can add two zeros to the original time series then do the convolution: \\[tsz = [0, 1, 4, 5, 3, 3, 4, 5, 6, 5, 3, 0]\\] How is covolution pattern matching/finding? In the above example, it easy to see that the filter \\(c\\) has the pattern [low, high low]. In the above example, at locations 2nd and 6th, we have \\[ts_2 = [4, 5, 3]; \\ ts_6 = [4, 2, 3]\\] The only difference is the value in the middle. It it straightforward to realize that the filter \\(c\\) helps to identify a pattern that has [low, high, low] since \\(c \\cdot ts_2 &gt; c \\cdot ts_6\\). But there is more. If we look at \\(tsv\\), we notice the two highest values are at locations 2nd and 8th: \\[ts_2 = [4, 5, 3]; \\ ts_8 = [3, 5, 3]\\] They both have the pattern of [low, high, low]. In other words, the filter \\(c=[1, 5, 1]\\) creates a spike in \\(tsv\\) when the pattern in \\(ts\\) is [low, high, low]. In a Euclidean space, the dot product of two vectors can be expressed as \\[a \\cdot b = ||a|| \\times ||b|| \\times \\cos{\\theta_{ab}}\\] where \\(||a||\\) and \\(||b||\\) are the magnitude of the two products and \\(\\theta_{ab}\\) is the angle between \\(a\\) and \\(b\\). Since \\(\\cos{(0)}=1\\), \\(\\cos{(\\pi/2)} = 0\\), and \\(\\cos{(\\pi)}=-1\\), the dot product not only measures the magnitudes of the two vectors, but also their correlations. Take an extreme example: when the angle between them is \\(\\pi\\), they are orthogonal and the dot product is equal to zero no matter the magnitude. To summarize what we have discussed so far, we say that covolution is cross-correlation. When the segment of the data is highly correlates to the filter, it creates a spike in value and hence indicates a certain pattern. Second, convolution reduces the number of parameters of the model. Let’s go back to the example above. Suppose you hypothesize that there is a 3-day pattern in the data, which actually prompted the use of a filter with size 3. If you want to look at all windows of size 3 in the data, you would be looking at 8 of such windows and a total of 24 parameters, 1 for each day in each window. By using convolution and the sliding filter, you only need 3 filters: the size of the filter. This is not much of a saving in our example, but imagine the case of images, and the difference is huge. By using the filter sliding through the data, we have stopped to care where the pattern happens, but only that it has happened. This is called translational invariance, which is important, again, in computer vision. Imagine you have two pictures of the same cat in the same posture from the same angle, except one of them the cat is on the floor and the other up on the table. It is the same cat. Your filter should be finding the cat, and it should not care where the cat is. Translational invariance is not as prominent in time-series data, but here is one example with our stock price data: suppose every time a stock’s price goes up by more than 10% in a single day, it will follow with a decline; but if the hike is less than 5%, it will follow with another hike. This is a pattern that a filter (or two) should be able to match. And it does not matter when (in analagous to where in CV) it happens. Third, and before we move on to code, we should introduce two related concepts in CNN: pooling and feature maps. Pooling reduces the size of the data. Continue with our example above with \\(ts\\) and \\(c\\). Suppose we do a full mode convolution, i.e., sliding \\(c\\) over \\(tsz\\), then we have the new convolved series as: \\[tsf = [9, 26, 32, 23, 22, 25, 17, 22, 31, 20]\\] we can perform max pooling on \\(tsf\\) to reduce its size to 5. What we do is to group every two numbers, then pick the highest number from the group: \\(tsfp = [\\{9, 26\\}, \\{32, 23\\}, \\{22, 25\\}, \\{17, 22\\}, \\{31, 20\\}]\\) \\(tsfp = [26, 32, 25, 22, 31]\\) The other way to do pooling is average pooling, but max pooling is more intuitive. At a high level, pooling, especially max pooling, does two things: it reduces the size of the data but preserves the “spikes”. In other words, this is another operation of “we do not care where/when as long as it happens”. By convention, even though pooling has reduced the size of the data, filter size remains the same. In other words, if we overlay \\(c\\) on \\(tsfp\\), at the first location (\\(tsfp_1 = [26, 32, 25]\\)), \\(c\\) is now finding patterns from the first 7 numbers in \\(ts\\). To see this, note that the value 25 in \\(tsfp_1\\) was calculated by \\[[1, 5, 1] \\cdot ts_5 \\Rightarrow [1, 5, 1] \\cdot [3, 4, 5]\\] where the value 5 in \\(ts_5\\) is the 7th value of \\(ts\\). In other words, with pooling and same size filters, CNN is able to see bigger and bigger “pictures” when data is passed through the convolution layers. However, it is important to increase the number of filters after each pooling until the size of the feature map is large enough. A feature map is a collection of features. It has a pictorial name because CNN was first developed for computer vision. The reason for the increasing size of feature map is straightforward: as data goes through the convolution layers, the filters are search wider and wider due to pooling. Increasing the number of features/filters would allow the CNN to look deeper. This helps to preserve information while transformation is happening. For time-series data, instead of a long time series, we can think of the data output from the convolution layers as a stack of many moments. After going through the covolution (and pooling) layers, the output is fed into some Dense layers just like ANN. In a way, we can think of CNN as two-stage feature engineering: covolution layers and Dense layers. 3.8 CNN in TensorFlow/Keras 3.9 Facebook Prophet https://facebook.github.io/prophet/ 3.10 Summary brief disucssion on self-supervised learning? 3.11 References https://www.udemy.com/course/time-series-analysis/ "],["regression-reconsidered.html", "Chapter 4 Regression Reconsidered", " Chapter 4 Regression Reconsidered "],["causal-inference-reconsidered.html", "Chapter 5 Causal Inference Reconsidered", " Chapter 5 Causal Inference Reconsidered "],["more-than-meets-the-eye.html", "Chapter 6 More than Meets the Eye", " Chapter 6 More than Meets the Eye "]]
